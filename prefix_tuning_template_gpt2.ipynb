{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d7325f-2424-4320-8dc1-571b1a810f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 1,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \". . . plays like somebody spliced random moments of a chris rock routine into what is otherwise a cliche-riddled but self-serious spy thriller . What is the sentiment expressed by the reviewer for the movie? \",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n",
      "<class 'openprompt.data_utils.utils.InputExample'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: LR=0.005, Soft Tokens=50, Warm-Up Steps=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 60it [00:00, 1340.52it/s]\n",
      "tokenizing: 1000it [00:00, 1841.05it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:59<00:00,  4.94s/it, loss=0.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:49<00:00,  4.15s/it, loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:57<00:00,  4.83s/it, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:55<00:00,  4.62s/it, loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:00<00:00,  5.08s/it, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:56<00:00,  4.74s/it, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:59<00:00,  4.92s/it, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:00<00:00,  5.05s/it, loss=0.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:03<00:00,  5.29s/it, loss=0.755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:57<00:00,  4.83s/it, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.4940\n",
      "Testing: LR=0.005, Soft Tokens=50, Warm-Up Steps=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 658.56it/s]\n",
      "tokenizing: 1000it [00:00, 1019.97it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:57<00:00,  4.75s/it, loss=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:49<00:00,  4.17s/it, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:47<00:00,  3.97s/it, loss=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:47<00:00,  3.94s/it, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:44<00:00,  3.73s/it, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:33<00:00,  2.80s/it, loss=0.705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:26<00:00,  2.24s/it, loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:24<00:00,  2.04s/it, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:32<00:00,  2.71s/it, loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:31<00:00,  2.60s/it, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.4670\n",
      "Testing: LR=0.005, Soft Tokens=50, Warm-Up Steps=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 1380.55it/s]\n",
      "tokenizing: 1000it [00:00, 1385.55it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:36<00:00,  3.01s/it, loss=0.918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:31<00:00,  2.65s/it, loss=0.723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:31<00:00,  2.59s/it, loss=0.784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:31<00:00,  2.60s/it, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:28<00:00,  2.40s/it, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:29<00:00,  2.49s/it, loss=0.745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:30<00:00,  2.55s/it, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:25<00:00,  2.15s/it, loss=0.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:29<00:00,  2.49s/it, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:30<00:00,  2.57s/it, loss=0.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.5330\n",
      "Testing: LR=0.005, Soft Tokens=100, Warm-Up Steps=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 1145.86it/s]\n",
      "tokenizing: 1000it [00:00, 1322.95it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:43<00:00,  3.65s/it, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:41<00:00,  3.44s/it, loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:42<00:00,  3.52s/it, loss=0.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:44<00:00,  3.69s/it, loss=0.721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:41<00:00,  3.46s/it, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it, loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:46<00:00,  3.85s/it, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:42<00:00,  3.58s/it, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:42<00:00,  3.54s/it, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:44<00:00,  3.73s/it, loss=0.831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.5330\n",
      "Testing: LR=0.005, Soft Tokens=100, Warm-Up Steps=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 945.83it/s]\n",
      "tokenizing: 1000it [00:00, 1477.44it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:42<00:00,  3.55s/it, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:39<00:00,  3.33s/it, loss=0.789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:35<00:00,  2.96s/it, loss=0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:34<00:00,  2.91s/it, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:38<00:00,  3.20s/it, loss=0.713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:39<00:00,  3.25s/it, loss=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:38<00:00,  3.20s/it, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:38<00:00,  3.24s/it, loss=0.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:40<00:00,  3.36s/it, loss=0.721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:41<00:00,  3.49s/it, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.4670\n",
      "Testing: LR=0.005, Soft Tokens=100, Warm-Up Steps=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 1077.80it/s]\n",
      "tokenizing: 1000it [00:00, 1192.99it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:41<00:00,  3.44s/it, loss=1.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:36<00:00,  3.04s/it, loss=0.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:37<00:00,  3.11s/it, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:39<00:00,  3.27s/it, loss=0.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:38<00:00,  3.18s/it, loss=0.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:39<00:00,  3.33s/it, loss=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:36<00:00,  3.08s/it, loss=0.741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:44<00:00,  3.72s/it, loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:44<00:00,  3.67s/it, loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:42<00:00,  3.55s/it, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.4670\n",
      "Tuning complete. Results saved to prefix_tuning_results_gpt2_6.json\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import LMTokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from openprompt.prompts import PrefixTuningTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from random import shuffle\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from openprompt.data_utils import InputExample\n",
    "import json\n",
    "dataset_path = \"/lustre/work/client/users/minhos/cache/datasets/p3_sciq_multiple_choice\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "label_map = {\"positive\": 0,\"negative\": 1}\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    if split == 'train':\n",
    "        raw_dataset[split] = raw_dataset[split].shuffle(seed=42).select(range(1000))\n",
    "    else:\n",
    "        raw_dataset[split] = raw_dataset[split].select(range(1000))\n",
    "    \n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        label_text = data[\"targets_pretokenized\"].strip().lower()\n",
    "        label_numeric = label_map.get(label_text, -1)\n",
    "        input_example = InputExample(text_a=data['inputs_pretokenized'], guid=idx, label=label_numeric)\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "sampler = FewShotSampler(num_examples_per_label=30)\n",
    "fewshot_data = sampler(dataset['train'], seed=42)\n",
    "\n",
    "# Load the T5 model\n",
    "from openprompt.plms import load_plm\n",
    "gpt_path = \"/lustre/work/client/users/minhos/models_for_supercomputer/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(gpt_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"prefix_tuning_results_gpt2_6.json\"\n",
    "results = []\n",
    "\n",
    "# Hyperparameter search ranges\n",
    "learning_rates = [0.005] # 0.0005, 0.001, 0.005\n",
    "num_soft_tokens = [50, 100] # 10, 50, 100\n",
    "warmup_steps = [10, 20, 25] #10, 20, 25\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for tokens in num_soft_tokens:\n",
    "        for warmup in warmup_steps:\n",
    "            print(f\"Testing: LR={lr}, Soft Tokens={tokens}, Warm-Up Steps={warmup}\")\n",
    "            \n",
    "            model = GPT2LMHeadModel.from_pretrained(gpt_path)\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(gpt_path)\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "            template = PrefixTuningTemplate(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                text='{\"placeholder\":\"text_a\"} {\"mask\"}',\n",
    "                num_token=10,  # Number of virtual tokens\n",
    "            )\n",
    "            \n",
    "            from openprompt.prompts import ManualVerbalizer\n",
    "            verbalizer = ManualVerbalizer(\n",
    "                tokenizer=tokenizer, \n",
    "                num_classes=2,  # Example: binary classification\n",
    "                label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"terrible\"]],\n",
    "                classes= [0, 1]\n",
    "            )\n",
    "            \n",
    "            wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "\n",
    "\n",
    "            prompt_model = PromptForClassification(plm=model,template=template, verbalizer=verbalizer, freeze_plm=True)\n",
    "            \n",
    "            \n",
    "            train_dataloader = PromptDataLoader(dataset=fewshot_data, template=template, tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=LMTokenizerWrapper, max_seq_length=480, decoder_max_length=3,\n",
    "                batch_size=5,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "                truncate_method=\"tail\")\n",
    "            \n",
    "            validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=template, tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=LMTokenizerWrapper, max_seq_length=480, decoder_max_length=3,\n",
    "                batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "                truncate_method=\"tail\")\n",
    "            \n",
    "            # Define loss function\n",
    "            loss_func = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Define optimizer for the prefix tuning parameters\n",
    "            optimizer_grouped_parameters = [{'params': [p for name, p in template.named_parameters() if 'raw_embedding' not in name]}]\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "            \n",
    "            # Define a learning rate scheduler\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=1000)\n",
    "\n",
    "\n",
    "            from tqdm import tqdm\n",
    "            \n",
    "            # Ensure model is in training mode\n",
    "            prompt_model.train()\n",
    "            \n",
    "            # Training parameters\n",
    "            num_epochs = 10\n",
    "            gradient_accumulation_steps = 1\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                total_loss = 0\n",
    "                pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "                \n",
    "                for step, inputs in enumerate(pbar):\n",
    "                    logits = prompt_model(inputs)\n",
    "                    labels = inputs['label']  # Ground-truth labels\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = loss_func(logits, labels)\n",
    "                    loss.backward()  # Backpropagation\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    pbar.set_postfix({\"loss\": total_loss / (step + 1)})\n",
    "            \n",
    "            \n",
    "            def evaluate(prompt_model, dataloader):\n",
    "                prompt_model.eval()  # Set the model to evaluation mode\n",
    "                total, correct = 0, 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs in dataloader:\n",
    "                        logits = prompt_model(inputs)\n",
    "                        preds = torch.argmax(logits, dim=-1)\n",
    "                        labels = inputs['label']\n",
    "                        \n",
    "                        total += len(labels)\n",
    "                        correct += (preds == labels).sum().item()\n",
    "                \n",
    "                accuracy = correct / total\n",
    "                return accuracy\n",
    "            \n",
    "            \n",
    "            # Validation after each epoch\n",
    "            val_accuracy = evaluate(prompt_model, validation_dataloader)\n",
    "            print(f\"Validation Accuracy after Epoch {epoch + 1}: {val_accuracy:.4f}\")\n",
    "            # Log results\n",
    "            result = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_soft_tokens\": tokens,\n",
    "                \"warmup_steps\": warmup,\n",
    "                \"final_loss\": total_loss / (10 * len(train_dataloader)),\n",
    "                \"accuracy\": val_accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Save intermediate results\n",
    "            with open(log_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "\n",
    "                    \n",
    "print(\"Tuning complete. Results saved to\", log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c98335-3488-41a6-93f8-d2153c799144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time the verbalizer label: label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]]\n",
    "# loss in the beginning: 0.829\n",
    "# loss after training 10 epochs: 0.632\n",
    "# Number of soft tokens: 10\n",
    "# warmup step: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy:0.5180\n",
    "\n",
    "\n",
    "# ----------\n",
    "# loss in the beginning: 0.955\n",
    "# loss after training 10 epochs: 0.698\n",
    "# warmup step: 10\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.005\n",
    "# accuracy:0.5330\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b109676-ffaf-42f7-a011-19f0b6d39d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
