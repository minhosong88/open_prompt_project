{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7325f-2424-4320-8dc1-571b1a810f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import LMTokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from openprompt.prompts import PrefixTuningTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from random import shuffle\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from openprompt.data_utils import InputExample\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/path/to/your/data/set\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Map textual labels to numeric labels\n",
    "label_map = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "# Prepare datasets\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    if split == 'train':\n",
    "        # Shuffle and select a subset for training\n",
    "        raw_dataset[split] = raw_dataset[split].shuffle(seed=42).select(range(1000))\n",
    "    else:\n",
    "        # Select a subset for validation\n",
    "        raw_dataset[split] = raw_dataset[split].select(range(1000))\n",
    "    \n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        label_text = data[\"targets_pretokenized\"].strip().lower()  # Extract label text\n",
    "        label_numeric = label_map.get(label_text, -1)  # Convert to numeric label\n",
    "        input_example = InputExample(text_a=data['inputs_pretokenized'], guid=idx, label=label_numeric)\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "# Few-shot sampling from training data\n",
    "sampler = FewShotSampler(num_examples_per_label=30)\n",
    "fewshot_data = sampler(dataset['train'], seed=42)\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "gpt_path = \"/path/to/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(gpt_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"prefix_tuning_results_gpt2.json\"\n",
    "results = []\n",
    "\n",
    "# Define hyperparameter search ranges\n",
    "learning_rates = [0.0005, 0.001, 0.005]  # Learning rates to test\n",
    "num_soft_tokens = [10, 50, 100]  # Number of soft tokens for prefix tuning\n",
    "warmup_steps = [10, 20, 25]  # Warm-up steps for learning rate scheduler\n",
    "\n",
    "# Loop through hyperparameter combinations\n",
    "for lr in learning_rates:\n",
    "    for tokens in num_soft_tokens:\n",
    "        for warmup in warmup_steps:\n",
    "            print(f\"Testing: LR={lr}, Soft Tokens={tokens}, Warm-Up Steps={warmup}\")\n",
    "            \n",
    "            # Reload model and tokenizer for each configuration\n",
    "            model = GPT2LMHeadModel.from_pretrained(gpt_path)\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(gpt_path)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Define the prefix tuning template\n",
    "            template = PrefixTuningTemplate(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                text='{\"placeholder\":\"text_a\"} {\"mask\"}',\n",
    "                num_token=tokens,  # Number of virtual tokens\n",
    "            )\n",
    "            \n",
    "            # Define a manual verbalizer\n",
    "            verbalizer = ManualVerbalizer(\n",
    "                tokenizer=tokenizer, \n",
    "                num_classes=2,  # Binary classification\n",
    "                label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"terrible\"]],\n",
    "                classes=[0, 1]\n",
    "            )\n",
    "            \n",
    "            # Initialize the prompt model\n",
    "            prompt_model = PromptForClassification(\n",
    "                plm=model, \n",
    "                template=template, \n",
    "                verbalizer=verbalizer, \n",
    "                freeze_plm=True\n",
    "            )\n",
    "            \n",
    "            # Create dataloaders\n",
    "            train_dataloader = PromptDataLoader(\n",
    "                dataset=fewshot_data, \n",
    "                template=template, \n",
    "                tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=LMTokenizerWrapper, \n",
    "                max_seq_length=480, \n",
    "                decoder_max_length=3,\n",
    "                batch_size=5, \n",
    "                shuffle=True, \n",
    "                teacher_forcing=False, \n",
    "                predict_eos_token=False,\n",
    "                truncate_method=\"tail\"\n",
    "            )\n",
    "            \n",
    "            validation_dataloader = PromptDataLoader(\n",
    "                dataset=dataset[\"validation\"], \n",
    "                template=template, \n",
    "                tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=LMTokenizerWrapper, \n",
    "                max_seq_length=480, \n",
    "                decoder_max_length=3,\n",
    "                batch_size=5, \n",
    "                shuffle=False, \n",
    "                teacher_forcing=False, \n",
    "                predict_eos_token=False,\n",
    "                truncate_method=\"tail\"\n",
    "            )\n",
    "            \n",
    "            # Define loss function\n",
    "            loss_func = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Define optimizer for prefix tuning parameters\n",
    "            optimizer_grouped_parameters = [{'params': [p for name, p in template.named_parameters() if 'raw_embedding' not in name]}]\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "            \n",
    "            # Define learning rate scheduler\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=1000)\n",
    "\n",
    "            from tqdm import tqdm\n",
    "            \n",
    "            # Set model to training mode\n",
    "            prompt_model.train()\n",
    "            \n",
    "            # Training parameters\n",
    "            num_epochs = 10\n",
    "            gradient_accumulation_steps = 1\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                total_loss = 0\n",
    "                pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "                \n",
    "                for step, inputs in enumerate(pbar):\n",
    "                    logits = prompt_model(inputs)  # Get model predictions\n",
    "                    labels = inputs['label']  # Ground-truth labels\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = loss_func(logits, labels)\n",
    "                    loss.backward()  # Backpropagation\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    pbar.set_postfix({\"loss\": total_loss / (step + 1)})\n",
    "            \n",
    "            # Define evaluation function\n",
    "            def evaluate(prompt_model, dataloader):\n",
    "                prompt_model.eval()  # Set model to evaluation mode\n",
    "                total, correct = 0, 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs in dataloader:\n",
    "                        logits = prompt_model(inputs)\n",
    "                        preds = torch.argmax(logits, dim=-1)  # Predicted class\n",
    "                        labels = inputs['label']\n",
    "                        total += len(labels)\n",
    "                        correct += (preds == labels).sum().item()\n",
    "                \n",
    "                return correct / total  # Compute accuracy\n",
    "            \n",
    "            # Validation after each epoch\n",
    "            val_accuracy = evaluate(prompt_model, validation_dataloader)\n",
    "            print(f\"Validation Accuracy after Epoch {epoch + 1}: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Log results\n",
    "            result = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_soft_tokens\": tokens,\n",
    "                \"warmup_steps\": warmup,\n",
    "                \"final_loss\": total_loss / (10 * len(train_dataloader)),\n",
    "                \"accuracy\": val_accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Save intermediate results\n",
    "            with open(log_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Tuning complete. Results saved to\", log_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87c36226",
   "metadata": {},
   "source": [
    "# Overview of Prefix Tuning for Sentiment Classification with GPT-2\n",
    "\n",
    "This code implements **prefix tuning** for sentiment classification using the OpenPrompt framework and a pre-trained GPT-2 model. It explores the use of soft prompts to fine-tune GPT-2 for a binary classification task in a **few-shot learning setting**. The script includes hyperparameter tuning to optimize the learning process and evaluates the model's performance across different configurations.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### 1. **Dataset Preparation**\n",
    "- Loads the dataset from disk and preprocesses it into the OpenPrompt-compatible `InputExample` format.\n",
    "- Maps sentiment labels (`positive`, `negative`) to numeric values for binary classification.\n",
    "- Applies a **few-shot sampling** strategy to simulate low-resource scenarios by selecting 30 examples per label.\n",
    "\n",
    "### 2. **Prefix Tuning**\n",
    "- Uses the `PrefixTuningTemplate` to introduce **soft tokens** that are added to the input text, enabling the model to adapt to the sentiment classification task without modifying the base GPT-2 model weights.\n",
    "- The number of soft tokens is adjustable and tested as part of the hyperparameter tuning.\n",
    "\n",
    "### 3. **Manual Verbalizer**\n",
    "- Maps the model's predictions to human-readable sentiment labels (`positive`, `negative`).\n",
    "- Uses a manually defined set of label words to guide the language model's output during classification.\n",
    "\n",
    "### 4. **Training Process**\n",
    "- Trains only the prefix-tuning parameters while keeping the GPT-2 model frozen, significantly reducing computational overhead.\n",
    "- Tracks training loss for each batch and adjusts learning rates using a linear scheduler with warm-up steps.\n",
    "\n",
    "### 5. **Hyperparameter Tuning**\n",
    "- Explores various combinations of:\n",
    "  - **Learning rates** (`0.0005`, `0.001`, `0.005`).\n",
    "  - **Number of soft tokens** (`10`, `50`, `100`).\n",
    "  - **Warm-up steps** (`10`, `20`, `25`).\n",
    "- Automates hyperparameter experimentation to identify the best-performing configuration.\n",
    "\n",
    "### 6. **Evaluation**\n",
    "- Validates the model's performance on a separate validation set after each training epoch.\n",
    "- Computes accuracy by comparing the model's predictions against ground-truth labels.\n",
    "\n",
    "### 7. **Result Logging**\n",
    "- Logs training loss, validation accuracy, and hyperparameter settings.\n",
    "- Saves results to a JSON file for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Known Limitation of GPT-2\n",
    "\n",
    "### **Verbose or Irrelevant Outputs**\n",
    "GPT-2 often generates verbose or irrelevant outputs instead of concise, task-relevant answers. This behavior arises from its general-purpose training and lack of fine-tuning for the specific task of sentiment classification. As a result, additional fine-tuning or prompt engineering is required to make GPT-2 outputs more aligned with task objectives.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "### 1. **Dataset Loading and Preprocessing**\n",
    "- The dataset is loaded and split into training and validation subsets.\n",
    "- Each data point is converted into an `InputExample` format compatible with OpenPrompt.\n",
    "\n",
    "### 2. **Few-Shot Sampling**\n",
    "- A small subset of the training data is sampled to simulate a low-resource environment, with balanced examples for each label.\n",
    "\n",
    "### 3. **Prefix Tuning Template**\n",
    "- Initializes a soft prompt using the `PrefixTuningTemplate`.\n",
    "- Virtual tokens are appended to the input text to steer the pre-trained GPT-2 model toward the classification task.\n",
    "\n",
    "### 4. **Training**\n",
    "- The prefix-tuning parameters are optimized using the AdamW optimizer.\n",
    "- A linear learning rate scheduler with warm-up steps ensures a stable optimization process.\n",
    "\n",
    "### 5. **Evaluation**\n",
    "- After each epoch, the model is evaluated on the validation set.\n",
    "- Accuracy metrics are calculated and logged for each hyperparameter configuration.\n",
    "\n",
    "### 6. **Hyperparameter Search**\n",
    "- The script iterates over different combinations of learning rates, soft token counts, and warm-up steps to find the optimal setup.\n",
    "- Each configuration is tested independently, and the results are logged for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **Few-Shot Learning**: Demonstrates how prefix tuning can effectively adapt pre-trained language models for classification tasks in low-resource settings.\n",
    "- **Sentiment Analysis**: Applies prefix tuning to a binary sentiment classification task (`positive` vs. `negative`).\n",
    "- **Prompt-Based Learning**: Highlights the flexibility of soft prompts for adapting language models to downstream tasks without full fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of Prefix Tuning\n",
    "\n",
    "1. **Parameter Efficiency**:\n",
    "   - Only optimizes a small number of parameters (soft prompts), reducing computational costs and memory requirements.\n",
    "\n",
    "2. **Low-Resource Adaptation**:\n",
    "   - Achieves strong performance with minimal labeled data, making it suitable for few-shot learning.\n",
    "\n",
    "3. **Modularity**:\n",
    "   - Easily integrates with pre-trained language models like GPT-2.\n",
    "   - Allows experimentation with different templates, verbalizers, and hyperparameter settings.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This implementation provides a practical demonstration of prefix tuning for sentiment classification using GPT-2. By combining soft prompts, manual verbalizers, and hyperparameter tuning, the script highlights the flexibility and efficiency of prompt-based learning for binary classification tasks in NLP. However, the inherent limitation of GPT-2's verbosity or irrelevance in output generation underscores the need for fine-tuning to align the model better with task-specific requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
