{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000e7bb-385e-4398-8688-a755d77369c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from openprompt.prompts import ManualTemplate, MixedTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from random import shuffle\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "dataset_path = \"/lustre/work/client/users/minhos/cache/datasets/p3_sciq_multiple_choice\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "dataset = {'validation':[]}\n",
    "\n",
    "raw_dataset['validation'] = raw_dataset['validation'].select(range(1000))\n",
    "for idx, data in enumerate(raw_dataset['validation']):\n",
    "    # Extract necessary fields\n",
    "    question = data[\"inputs_pretokenized\"]  # The question + context\n",
    "    choices = data[\"answer_choices\"]       # List of answer choices\n",
    "    correct_answer = data[\"targets_pretokenized\"].strip()  # Correct answer text\n",
    "    \n",
    "    # Find the index of the correct answer\n",
    "    correct_index = choices.index(correct_answer) if correct_answer in choices else -1\n",
    "    if correct_index == -1:\n",
    "        print(f\"Correct answer not found in choices for index {idx}\")\n",
    "        continue\n",
    "    formatted_choices = \",\".join(choices)\n",
    "    # Create an InputExample\n",
    "    input_example = InputExample(\n",
    "        text_a=question,\n",
    "        guid=idx,\n",
    "        label=correct_index,\n",
    "        meta={\"choices\": formatted_choices}\n",
    "    )\n",
    "\n",
    "    dataset['validation'].append(input_example)\n",
    "\n",
    "print(dataset['validation'][0])\n",
    "print(type(dataset['validation'][0]))\n",
    "\n",
    "# Load the T5 model\n",
    "t5_path = \"/lustre/work/client/users/minhos/models_for_supercomputer/t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "\n",
    "# Setup Template for the evaluation\n",
    "template = ManualTemplate(\n",
    "    tokenizer=tokenizer,\n",
    "    text='{\"placeholder\":\"text_a\"}{\"mask\"}',\n",
    ")\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"qa_manual_template_t5_diff_data.json\"\n",
    "results = []\n",
    "\n",
    "# Iterate dataset by generating a new verbalizer for each dataset\n",
    "for data in dataset['validation']:\n",
    "\n",
    "    def format_labels(choices):\n",
    "    # Split the string into a list, strip whitespace from each choice\n",
    "        return [choice.strip() for choice in choices.split(\",\")]\n",
    "            \n",
    "        \n",
    "    def create_dynamic_verbalizer(choices, tokenizer):\n",
    "        formatted_labels = format_labels(choices)\n",
    "        # Tokenize each label word\n",
    "    \n",
    "        return ManualVerbalizer(\n",
    "            tokenizer=tokenizer,\n",
    "            num_classes=len(format_labels(choices)),\n",
    "            label_words= [[label] for label in formatted_labels]a\n",
    "\n",
    "        )\n",
    "\n",
    "    choices = data.meta[\"choices\"]\n",
    "    formatted_labels = format_labels(choices)\n",
    "\n",
    "    verbalizer = create_dynamic_verbalizer(choices, tokenizer)\n",
    "    prompt_model = PromptForClassification(\n",
    "        plm=model,\n",
    "        template=template,\n",
    "        verbalizer=verbalizer,\n",
    "        freeze_plm=True, \n",
    "    )\n",
    "    validation_dataloader = PromptDataLoader(\n",
    "        dataset=[data],\n",
    "        template=template,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "        decoder_max_length=50,max_seq_length=480,\n",
    "        batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "        truncate_method=\"tail\",\n",
    "    )\n",
    "\n",
    "    # With this set up, the prediction becomes <pad> </s> <ukn> or ''  \n",
    "    def evaluate_single_example(prompt_model, dataloader, tokenizer):\n",
    "        prompt_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in dataloader:\n",
    "                logits = prompt_model(inputs)\n",
    "                for idx, label in enumerate(verbalizer.label_words):\n",
    "                   print(f\"Logit for '{label}': {logits[:, idx].item()}\")\n",
    "                generated_ids = torch.argmax(logits, dim=-1)  # Get token IDs\n",
    "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_class = generated_ids.item()\n",
    "                return generated_ids, generated_text, predicted_class, data.label\n",
    "    \n",
    "    def evaluate_single_example(prompt_model, dataloader, tokenizer):\n",
    "        prompt_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in dataloader:\n",
    "                logits = prompt_model(inputs)\n",
    "                for idx, label in enumerate(verbalizer.label_words):\n",
    "                    print(f\"Logit for '{label}': {logits[:, idx].item()}\")\n",
    "                aggregated_logits = [\n",
    "                    torch.max(torch.tensor([logits[:, tokenizer.convert_tokens_to_ids(token)] for token in label]))\n",
    "                    for label in verbalizer.label_words\n",
    "                ]\n",
    "                print(f\"Logit for class ({verbalizer.label_words[0]}): {aggregated_logits[0]}\")\n",
    "                generated_ids = torch.argmax(torch.tensor(aggregated_logits), dim=-1)  # Get token IDs\n",
    "                predicted_class = generated_ids.item()\n",
    "                generated_text = \" \".join(verbalizer.label_words[predicted_class])\n",
    "                return generated_ids, generated_text, predicted_class, data.label\n",
    "\n",
    "    generated_ids, generated_text, predicted_class, label = evaluate_single_example(\n",
    "        prompt_model, validation_dataloader, tokenizer\n",
    "    )\n",
    "    correct = predicted_class == label\n",
    "    results.append({\n",
    "        \"index\": idx, \n",
    "        \"generated_text\": generated_text,\n",
    "        \"predicted_class\": predicted_class, \n",
    "        \"true_class\": label, \n",
    "        \"correct\": correct\n",
    "    })\n",
    "\n",
    "    \n",
    "    print(f\"Example {idx + 1}/{len(dataset['validation'])}: Generated 'id:{generated_ids}, {generated_text}', Predicted {predicted_class}, True {label} - \"\n",
    "          f\"{'Correct' if correct else 'Incorrect'}\" )\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save results to JSON\n",
    "with open(log_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
