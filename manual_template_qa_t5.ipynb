{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000e7bb-385e-4398-8688-a755d77369c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from openprompt.prompts import ManualTemplate, MixedTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from random import shuffle\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Path to the dataset and loading the raw dataset\n",
    "dataset_path = \"path/to/your/data/set\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Initialize the dataset dictionary to store the validation set\n",
    "dataset = {'validation': []}\n",
    "\n",
    "# Limit validation dataset to 1000 examples for testing\n",
    "raw_dataset['validation'] = raw_dataset['validation'].select(range(1000))\n",
    "\n",
    "# Process each example in the validation set\n",
    "for idx, data in enumerate(raw_dataset['validation']):\n",
    "    # Extract necessary fields\n",
    "    question = data[\"inputs_pretokenized\"]  # The question and context\n",
    "    choices = data[\"answer_choices\"]        # List of answer choices\n",
    "    correct_answer = data[\"targets_pretokenized\"].strip()  # Correct answer text\n",
    "\n",
    "    # Identify the correct answer's index within the choices\n",
    "    correct_index = choices.index(correct_answer) if correct_answer in choices else -1\n",
    "    if correct_index == -1:\n",
    "        # Skip examples where the correct answer is missing\n",
    "        print(f\"Correct answer not found in choices for index {idx}\")\n",
    "        continue\n",
    "    \n",
    "    # Create an InputExample for OpenPrompt\n",
    "    formatted_choices = \",\".join(choices)\n",
    "    input_example = InputExample(\n",
    "        text_a=question,\n",
    "        guid=idx,\n",
    "        label=correct_index,\n",
    "        meta={\"choices\": formatted_choices}\n",
    "    )\n",
    "\n",
    "    # Append the processed example to the validation dataset\n",
    "    dataset['validation'].append(input_example)\n",
    "\n",
    "# Display the first validation example and its type\n",
    "print(dataset['validation'][0])\n",
    "print(type(dataset['validation'][0]))\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "t5_path = \"/path/to/t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "# Setup a ManualTemplate for processing the input data\n",
    "template = ManualTemplate(\n",
    "    tokenizer=tokenizer,\n",
    "    text='{\"placeholder\":\"text_a\"}{\"mask\"}',\n",
    ")\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"results.json\"\n",
    "results = []\n",
    "\n",
    "# Iterate through the validation dataset for evaluation\n",
    "for data in dataset['validation']:\n",
    "\n",
    "    # Helper function to format label choices\n",
    "    def format_labels(choices):\n",
    "        # Split the string into a list, strip whitespace from each choice\n",
    "        return [choice.strip() for choice in choices.split(\",\")]\n",
    "            \n",
    "    # Function to dynamically create a verbalizer based on the choices\n",
    "    def create_dynamic_verbalizer(choices, tokenizer):\n",
    "        formatted_labels = format_labels(choices)\n",
    "        return ManualVerbalizer(\n",
    "            tokenizer=tokenizer,\n",
    "            num_classes=len(formatted_labels),\n",
    "            label_words=[[label] for label in formatted_labels]\n",
    "        )\n",
    "\n",
    "    # Get the choices and format labels\n",
    "    choices = data.meta[\"choices\"]\n",
    "    formatted_labels = format_labels(choices)\n",
    "\n",
    "    # Create a verbalizer for the current example\n",
    "    verbalizer = create_dynamic_verbalizer(choices, tokenizer)\n",
    "\n",
    "    # Initialize the PromptForClassification model\n",
    "    prompt_model = PromptForClassification(\n",
    "        plm=model,\n",
    "        template=template,\n",
    "        verbalizer=verbalizer,\n",
    "        freeze_plm=True,  # Freezes the pre-trained language model\n",
    "    )\n",
    "\n",
    "    # Prepare the dataloader for the validation example\n",
    "    validation_dataloader = PromptDataLoader(\n",
    "        dataset=[data],\n",
    "        template=template,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "        decoder_max_length=50,\n",
    "        max_seq_length=480,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        teacher_forcing=False,\n",
    "        predict_eos_token=False,\n",
    "        truncate_method=\"tail\",\n",
    "    )\n",
    "\n",
    "    # Function to evaluate a single example\n",
    "    def evaluate_single_example(prompt_model, dataloader, tokenizer):\n",
    "        prompt_model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs in dataloader:\n",
    "                # Get the logits from the model\n",
    "                logits = prompt_model(inputs)\n",
    "                # Decode the predictions\n",
    "                generated_ids = torch.argmax(logits, dim=-1)\n",
    "                predicted_class = generated_ids.item()\n",
    "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                return generated_ids, generated_text, predicted_class, data.label\n",
    "\n",
    "    # Evaluate the example and store the results\n",
    "    generated_ids, generated_text, predicted_class, label = evaluate_single_example(\n",
    "        prompt_model, validation_dataloader, tokenizer\n",
    "    )\n",
    "    correct = predicted_class == label\n",
    "    results.append({\n",
    "        \"index\": idx, \n",
    "        \"generated_text\": generated_text,\n",
    "        \"predicted_class\": predicted_class, \n",
    "        \"true_class\": label, \n",
    "        \"correct\": correct\n",
    "    })\n",
    "\n",
    "    # Log the result of the current example\n",
    "    print(f\"Example {idx + 1}/{len(dataset['validation'])}: Generated 'id:{generated_ids}, {generated_text}', Predicted {predicted_class}, True {label} - \"\n",
    "          f\"{'Correct' if correct else 'Incorrect'}\")\n",
    "\n",
    "# Compute the overall accuracy\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(log_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e951eb",
   "metadata": {},
   "source": [
    "# Multiple Choice QA with OpenPrompt Framework\n",
    "\n",
    "This project demonstrates an approach to evaluating multiple-choice QA tasks using the OpenPrompt framework, addressing its limitations when handling tasks that require dynamic target labels per input. The key challenge lies in OpenPrompt’s inability to train directly on multiple-choice QA due to the nature of dynamically assigned target labels for each input. This README provides a detailed explanation of the methodology and rationale behind the implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "OpenPrompt is a flexible framework for prompt-based learning with large language models (LLMs). However, **multiple-choice QA tasks** pose a unique challenge:\n",
    "- Each input requires a dynamically generated **target label** based on the possible answer choices.\n",
    "- OpenPrompt is **not trainable** for such tasks because it assumes fixed verbalizers and static labels, which do not apply in this case.\n",
    "\n",
    "To overcome this, we developed an **evaluation-only approach** that dynamically creates verbalizers and dataloaders for each example, enabling accurate assessment of the LLM's performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution Overview\n",
    "\n",
    "The implementation focuses on **evaluating multiple-choice QA tasks** without training, leveraging the flexibility of OpenPrompt while adhering to its limitations.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Dynamic Target Labeling**:\n",
    "   - For each example, extract the question, possible answer choices, and the correct answer.\n",
    "   - Dynamically create a verbalizer that maps each choice to a unique label.\n",
    "\n",
    "2. **Evaluation Without Training**:\n",
    "   - Use OpenPrompt’s `PromptForClassification` to run the evaluation in an **inference-only mode**.\n",
    "   - Dynamically generate a dataloader for each example, ensuring compatibility with the framework.\n",
    "\n",
    "3. **Granular Example Evaluation**:\n",
    "   - Process each example individually using a modular `evaluate_single_example` function.\n",
    "   - This function dynamically creates a verbalizer and evaluates the model's output for the given input.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Features\n",
    "\n",
    "- **Dynamic Verbalizer Creation**:\n",
    "  Each example has a unique verbalizer that maps answer choices to label words, enabling multiple-choice QA evaluation.\n",
    "\n",
    "- **Example-Level Evaluation**:\n",
    "  The implementation avoids training limitations by processing and evaluating each example independently.\n",
    "\n",
    "- **Accurate Logging and Debugging**:\n",
    "  - Logs logits for each label to facilitate detailed analysis of the model’s predictions.\n",
    "  - Records the predicted class, generated text, and accuracy for each example.\n",
    "\n",
    "- **Overall Metrics**:\n",
    "  Computes overall accuracy based on the results of individual example evaluations.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "- Extract and preprocess the dataset for multiple-choice QA.\n",
    "- Dynamically adjust the dataset to match OpenPrompt’s required format.\n",
    "\n",
    "### 2. Template Definition\n",
    "- Define a manual template to structure the input text for the LLM:\n",
    "  ```json\n",
    "  {\"placeholder\":\"text_a\"}{\"mask\"}\n",
    "  ```\n",
    "- This template enables OpenPrompt to understand and process the input data effectively.\n",
    "### 3. Verbalizer Creation\n",
    "- A dynamic verbalizer maps each answer choice to a unique token for prediction.\n",
    "- Example\n",
    "  ```python\n",
    "  ManualVerbalizer(\n",
    "    tokenizer=tokenizer,\n",
    "    num_classes=len(choices),\n",
    "    label_words=[[label] for label in formatted_labels]\n",
    "  )\n",
    "  ```\n",
    "### 4. Evaluation Function\n",
    "- `evaluate_single_example`: A reusable function that encapsulates the logic for evaluating one input at a time.\n",
    "- Handles the following:\n",
    " - Dynamic verbalizer and dataloader creation.\n",
    " - Logits evaluation for each label.\n",
    " - Decoding and interpreting the model’s predictions.\n",
    "\n",
    "## Limitations of OpenPrompt Framework\n",
    "\n",
    "The following challenges highlight the necessity of this approach:\n",
    "\n",
    "1. **Dynamic Target Labels**:\n",
    "   - Multiple-choice QA requires new target labels for each input, which OpenPrompt does not inherently support during training.\n",
    "\n",
    "2. **Non-Trainable Nature**:\n",
    "   - OpenPrompt is designed for evaluation or few-shot learning, making tasks like multiple-choice QA incompatible with traditional training paradigms.\n",
    "\n",
    "3. **Framework Constraints**:\n",
    "   - Fixed verbalizers and dataloaders do not accommodate the variability required by multiple-choice QA tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## How This Implementation Overcomes Limitations\n",
    "\n",
    "This project circumvents the above limitations by:\n",
    "\n",
    "- Dynamically creating verbalizers and dataloaders for each input example.\n",
    "- Using OpenPrompt in **inference-only mode**, focusing solely on evaluation without requiring training.\n",
    "- Adopting a modular, example-level evaluation approach that aligns with the framework’s strengths while bypassing its constraints.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
