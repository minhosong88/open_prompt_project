{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4cf13e-f0ed-4972-b69c-4a499af80d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 1,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \". . . plays like somebody spliced random moments of a chris rock routine into what is otherwise a cliche-riddled but self-serious spy thriller . What is the sentiment expressed by the reviewer for the movie? \",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n",
      "<class 'openprompt.data_utils.utils.InputExample'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 1361.84it/s]\n",
      "tokenizing: 1000it [00:00, 1516.01it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from openprompt.prompts import PrefixTuningTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Setting Dataset \n",
    "dataset_path = \"/lustre/work/client/users/minhos/cache/datasets/p3_rotten_tomato\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "# Load the T5 model\n",
    "t5_path = \"/lustre/work/client/users/minhos/models_for_supercomputer/t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare dataset to feed dataloader\n",
    "label_map = {\"positive\": 0,\"negative\": 1}\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    raw_dataset[split] = raw_dataset[split].shuffle(seed=42).select(range(1000))\n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        label_text = data[\"targets_pretokenized\"].strip().lower()\n",
    "        label_numeric = label_map.get(label_text, -1)\n",
    "        input_example = InputExample(text_a = data['inputs_pretokenized'], guid=idx, label=label_numeric)\n",
    "        dataset[split].append(input_example)\n",
    "\n",
    "sampler = FewShotSampler(num_examples_per_label=30)\n",
    "fewshot_data = sampler(dataset['train'], seed=42)\n",
    "\n",
    "\n",
    "# Prepare Template, Verbalizer, Model, Dataloader\n",
    "template = PrefixTuningTemplate(model=model, tokenizer=tokenizer,num_token=50)\n",
    "verbalizer = ManualVerbalizer(\n",
    "    tokenizer=tokenizer, \n",
    "    num_classes=2,  # Example: binary classification\n",
    "    label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]],\n",
    "    classes=[0, 1]\n",
    ")\n",
    "\n",
    "prompt_model = PromptForClassification(plm=model,template=template, verbalizer=verbalizer, freeze_plm=True)\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=fewshot_data, template=template, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=T5TokenizerWrapper, max_seq_length=480, decoder_max_length=3,\n",
    "    batch_size=5,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=template, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=T5TokenizerWrapper, max_seq_length=480, decoder_max_length=3,\n",
    "    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer for the prefix tuning parameters\n",
    "optimizer_grouped_parameters = [{'params': [p for name, p in template.named_parameters() if 'raw_embedding' not in name]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=20, num_training_steps=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7d9c2-c932-4294-93b1-15f1cbfa5eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbd7a738-cf0c-4adb-a483-914ffb1091dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution: Counter({0: 30, 1: 30})\n",
      "Label Distribution: Counter({1: 501, 0: 499})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label_distribution = Counter([example.label for example in fewshot_data])\n",
    "print(\"Label Distribution:\", label_distribution)\n",
    "from collections import Counter\n",
    "label_distribution_val = Counter([example.label for example in dataset[\"validation\"]])\n",
    "print(\"Label Distribution:\", label_distribution_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51c73fa-a55e-475d-9742-619d0b769e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:20<00:00,  1.74s/it, loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:16<00:00,  1.37s/it, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:15<00:00,  1.29s/it, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:16<00:00,  1.34s/it, loss=0.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:16<00:00,  1.37s/it, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:14<00:00,  1.24s/it, loss=0.586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:14<00:00,  1.24s/it, loss=0.575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:14<00:00,  1.18s/it, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:15<00:00,  1.26s/it, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:16<00:00,  1.37s/it, loss=0.425]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the model to training mode\n",
    "prompt_model.train()\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 10  # Define the number of epochs\n",
    "gradient_accumulation_steps = 1  # Define gradient accumulation if needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    \n",
    "    for step, inputs in enumerate(pbar):\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']  # Ground-truth labels\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        # Optimizer step\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": total_loss / (step + 1)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fbe2d4-8502-4cbd-afc6-b6543d4ac955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 1: 0.8480\n"
     ]
    }
   ],
   "source": [
    "def evaluate(prompt_model, dataloader):\n",
    "    prompt_model.eval()  # Set the model to evaluation mode\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in dataloader:\n",
    "            logits = prompt_model(inputs)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            labels = inputs['label']\n",
    "            \n",
    "            total += len(labels)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    # Training steps as defined above\n",
    "    \n",
    "    # Validation after each epoch\n",
    "    val_accuracy = evaluate(prompt_model, validation_dataloader)\n",
    "    print(f\"Validation Accuracy after Epoch {epoch + 1}: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6623eee0-bd3b-4021-9b8f-04248bc58a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time the verbalizer label: label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]]\n",
    "# loss in the beginning: 1.55\n",
    "# loss after training 10 epochs: 1.26\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=3e-5\n",
    "# accuracy: 0.5010\n",
    "\n",
    "# -------------------------- increase the sample from 30 to 100 ---------------------------------------------------------------------\n",
    "\n",
    "# This time the verbalizer label: label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]]\n",
    "# loss in the beginning: 1.55\n",
    "# loss after training 10 epochs: 0.443\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 100 \n",
    "# learning rate=3e-5\n",
    "# accuracy:0.8310\n",
    "\n",
    "#------------------------------------------lower the number of samples back to 30 ----------------------------------------------------------\n",
    "#------------------------------------------number of soft token increased from 10 to 100 ---------------------------------------------------\n",
    "\n",
    "# This time the verbalizer label: label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]]\n",
    "# loss in the beginning: 1.51\n",
    "# loss after training 10 epochs: 1.33\n",
    "# Number of soft tokens: 100\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=3e-5\n",
    "# accuracy: 0.4990\n",
    "\n",
    "#-------------------------------------------lowered learning rate from 3e-5 to 1e-4 ---------------------------------------------------\n",
    "\n",
    "# This time the verbalizer label: label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]]\n",
    "# loss in the beginning: 1.57\n",
    "# loss after training 10 epochs: 0.699\n",
    "# Number of soft tokens: 100\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.5360\n",
    "\n",
    "## so far learning warm up was 500, maybe better to lower it considering the size of the samples (warm-up: 10)\n",
    "\n",
    "#----------------------------------warmup step change from 500 to 10 --------------------------------------------------\n",
    "\n",
    "# This time the verbalizer label: label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"awful\"]]\n",
    "# loss in the beginning: 1.41\n",
    "# loss after training 10 epochs: 0.404\n",
    "# Number of soft tokens: 100\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.8470\n",
    "\n",
    "#----------------------------------------soft token lowered from 100 to 10 -------------------------------------\n",
    "\n",
    "# loss in the beginning: 1.43\n",
    "# loss after training 10 epochs: 0.353\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.8500\n",
    "\n",
    "#----------------------------------------learning rate adjustment-----------------------------------------------\n",
    "# loss in the beginning: 1.43\n",
    "# loss after training 10 epochs: 0.721\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.05\n",
    "# accuracy: 0.5010\n",
    "\n",
    "\n",
    "# loss in the beginning: 1.16\n",
    "# loss after training 10 epochs: 0.75\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.01\n",
    "# accuracy: 0.5010\n",
    "\n",
    "\n",
    "# loss in the beginning: 1.23\n",
    "# loss after training 10 epochs: 0.769\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.005\n",
    "# accuracy: 0.4990\n",
    "\n",
    "\n",
    "# loss in the beginning: 1.14\n",
    "# loss after training 10 epochs: 0.773\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.001\n",
    "# accuracy: 0.5010\n",
    "\n",
    "\n",
    "# loss in the beginning: 1.18\n",
    "# loss after training 10 epochs: 0.253\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.0005\n",
    "# accuracy: 0.8170\n",
    "\n",
    "#------------------------increase warm up step from 10 to 20 ---------------------------------------------\n",
    "\n",
    "# loss in the beginning: 1.33\n",
    "# loss after training 10 epochs: 0.196\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.0005\n",
    "# accuracy: 0.8200\n",
    "\n",
    "#------------------------increase warm up step from 20 to 25 ---------------------------------------------\n",
    "# loss in the beginning: 1.38\n",
    "# loss after training 10 epochs: 0.688\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=0.0005\n",
    "# accuracy: 0.7850\n",
    "\n",
    "#------------------------Lower learning rate back to 1e-4 with warm up step 25  ---------------------------------------------\n",
    "# loss in the beginning: 1.39\n",
    "# loss after training 10 epochs: 0.494\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.8490\n",
    "\n",
    "#------------------------Lower learning rate back to 1e-4 with warm up step 20  ---------------------------------------------\n",
    "\n",
    "# loss in the beginning: 1.54\n",
    "# loss after training 10 epochs: 0.415\n",
    "# Number of soft tokens: 10\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.8490\n",
    "\n",
    "#--------------------Increased the number of soft token from 10 to 100\n",
    "\n",
    "# loss in the beginning: 1.51\n",
    "# loss after training 10 epochs: 0.526\n",
    "# Number of soft tokens: 100\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.8360\n",
    "\n",
    "#------------------lowered the number of soft toekn from 100 to 50\n",
    "# loss in the beginning: 1.44\n",
    "# loss after training 10 epochs: 0.425\n",
    "# Number of soft tokens: 50\n",
    "# Number of samples per label: 30 \n",
    "# learning rate=1e-4\n",
    "# accuracy: 0.8480\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
