{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571142d5-4834-4851-b8f5-ae185cc2533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 1,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \". . . plays like somebody spliced random moments of a chris rock routine into what is otherwise a cliche-riddled but self-serious spy thriller . What is the sentiment expressed by the reviewer for the movie? \",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n",
      "<class 'openprompt.data_utils.utils.InputExample'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: LR=0.0001, Soft Tokens=100, Warm-Up Steps=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 60it [00:00, 383.76it/s]\n",
      "tokenizing: 1000it [00:01, 922.41it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:17<00:00,  6.42s/it, loss=0.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:57<00:00,  4.83s/it, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:57<00:00,  4.80s/it, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:57<00:00,  4.83s/it, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:03<00:00,  5.26s/it, loss=0.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:02<00:00,  5.18s/it, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:56<00:00,  4.67s/it, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:02<00:00,  5.22s/it, loss=0.0476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:58<00:00,  4.90s/it, loss=0.00165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:00<00:00,  5.01s/it, loss=0.00611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.6900\n",
      "Testing: LR=0.0001, Soft Tokens=100, Warm-Up Steps=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 60it [00:00, 745.71it/s]\n",
      "tokenizing: 1000it [00:00, 1098.77it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:01<00:00,  5.13s/it, loss=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:58<00:00,  4.88s/it, loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:06<00:00,  5.54s/it, loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:08<00:00,  5.67s/it, loss=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:07<00:00,  5.58s/it, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:58<00:00,  4.91s/it, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:08<00:00,  5.68s/it, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:01<00:00,  5.15s/it, loss=0.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:00<00:00,  5.07s/it, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [01:08<00:00,  5.75s/it, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Epoch 10: 0.6110\n",
      "Tuning complete. Results saved to prefix_tuning_results_roberta_6.json\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import PtuningTemplate\n",
    "from openprompt.plms import MLMTokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import InputExample\n",
    "from random import shuffle\n",
    "from transformers import RobertaTokenizer,RobertaForMaskedLM\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "\n",
    "\n",
    "dataset_path = \"/lustre/work/client/users/minhos/cache/datasets/p3_rotten_tomato\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "label_map = {\"positive\": 0,\"negative\": 1}\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    raw_dataset[split] = raw_dataset[split].shuffle(seed=42).select(range(1000))\n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        label_text = data[\"targets_pretokenized\"].strip().lower()\n",
    "        label_numeric = label_map.get(label_text, -1)\n",
    "        input_example = InputExample(text_a = data['inputs_pretokenized'], guid=idx, label=label_numeric)\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "sampler = FewShotSampler(num_examples_per_label=30)\n",
    "fewshot_data = sampler(dataset['train'], seed=42)\n",
    "\n",
    "# Load the RoBERTa model and tokenizer\n",
    "roberta_path = \"/lustre/work/client/users/minhos/models_for_supercomputer/facebook_roberta-base\"\n",
    "model = RobertaForMaskedLM.from_pretrained(roberta_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(roberta_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"prefix_tuning_results_roberta_6.json\"\n",
    "results = []\n",
    "\n",
    "# Hyperparameter search ranges\n",
    "learning_rates = [1e-4] # 1e-4, 3e-5\n",
    "num_soft_tokens = [100] # 10, 50, 100\n",
    "warmup_steps = [20, 25] # 10, 20, 25\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for tokens in num_soft_tokens:\n",
    "        for warmup in warmup_steps:\n",
    "            print(f\"Testing: LR={lr}, Soft Tokens={tokens}, Warm-Up Steps={warmup}\")\n",
    "\n",
    "            model = RobertaForMaskedLM.from_pretrained(roberta_path)\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(roberta_path)\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "\n",
    "            template = PtuningTemplate(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                text='{\"placeholder\":\"text_a\"} {\"mask\"}',\n",
    "                prompt_encoder_type=\"lstm\",  # Choose between \"lstm\" or \"mlp\"\n",
    "            )\n",
    "\n",
    "            verbalizer = ManualVerbalizer(\n",
    "                tokenizer=tokenizer, \n",
    "                num_classes=2,  # Example: binary classification\n",
    "                label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"terrible\"]],\n",
    "                classes= [0, 1]\n",
    "            )\n",
    "\n",
    "            prompt_model = PromptForClassification(\n",
    "                plm=model,\n",
    "                template=template,\n",
    "                verbalizer=verbalizer,\n",
    "                freeze_plm=True,  # Set to False if you want to fine-tune RoBERTa\n",
    "            )\n",
    "\n",
    "\n",
    "            train_dataloader = PromptDataLoader(\n",
    "                dataset=fewshot_data,\n",
    "                template=template,\n",
    "                tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=MLMTokenizerWrapper,\n",
    "                max_seq_length=480, decoder_max_length=3,\n",
    "                batch_size=5,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "                truncate_method=\"tail\",\n",
    "            )\n",
    "            \n",
    "            validation_dataloader = PromptDataLoader(\n",
    "                dataset=dataset[\"validation\"],\n",
    "                template=template,\n",
    "                tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=MLMTokenizerWrapper,\n",
    "                decoder_max_length=3,\n",
    "                batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "                truncate_method=\"tail\",\n",
    "            )\n",
    "\n",
    "            # Unfreeze parameters and prepare grouped parameters for the optimizer\n",
    "            optimizer_grouped_parameters = []\n",
    "\n",
    "            for name, param in prompt_model.named_parameters():\n",
    "                if not param.requires_grad and param.dtype in [torch.float32, torch.float64, torch.complex64, torch.complex128]:\n",
    "                    param.requires_grad = True   \n",
    "                optimizer_grouped_parameters.append({'params': param})\n",
    "\n",
    "            # Define optimizer for the prefix tuning parameters\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "            loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # Define a learning rate scheduler\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=1000)\n",
    "            # Ensure model is in training mode\n",
    "            prompt_model.train()\n",
    "\n",
    "            # Training loop\n",
    "            num_epochs = 10\n",
    "            gradient_accumulation_steps = 1\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                total_loss = 0\n",
    "                pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "                \n",
    "                for step, inputs in enumerate(pbar):\n",
    "                    logits = prompt_model(inputs)\n",
    "                    labels = inputs['label']  # Ground-truth labels\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = loss_func(logits, labels)\n",
    "                    loss.backward()  # Backpropagation\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    pbar.set_postfix({\"loss\": total_loss / (step + 1)})\n",
    "                    \n",
    "            def evaluate(prompt_model, dataloader):\n",
    "                prompt_model.eval()  # Set the model to evaluation mode\n",
    "                total, correct = 0, 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs in dataloader:\n",
    "                        logits = prompt_model(inputs)\n",
    "                        preds = torch.argmax(logits, dim=-1)\n",
    "                        labels = inputs['label']\n",
    "                        \n",
    "                        total += len(labels)\n",
    "                        correct += (preds == labels).sum().item()\n",
    "                \n",
    "                accuracy = correct / total\n",
    "                return accuracy\n",
    "                \n",
    "            # Validation after each epoch\n",
    "            val_accuracy = evaluate(prompt_model, validation_dataloader)\n",
    "            print(f\"Validation Accuracy after Epoch {epoch + 1}: {val_accuracy:.4f}\")\n",
    "            # Log results\n",
    "            result = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_soft_tokens\": tokens,\n",
    "                \"warmup_steps\": warmup,\n",
    "                \"final_loss\": total_loss / (10 * len(train_dataloader)),\n",
    "                \"accuracy\": val_accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Save intermediate results\n",
    "            with open(log_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "print(\"Tuning complete. Results saved to\", log_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
