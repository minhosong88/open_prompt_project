{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571142d5-4834-4851-b8f5-ae185cc2533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.prompts import PtuningTemplate\n",
    "from openprompt.plms import MLMTokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import InputExample\n",
    "from random import shuffle\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/path/to/your/data/set\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Map textual labels to numeric labels\n",
    "label_map = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "# Prepare datasets\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    # Shuffle and limit the dataset for efficiency\n",
    "    raw_dataset[split] = raw_dataset[split].shuffle(seed=42).select(range(1000))\n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        label_text = data[\"targets_pretokenized\"].strip().lower()  # Extract label text\n",
    "        label_numeric = label_map.get(label_text, -1)  # Convert to numeric label\n",
    "        input_example = InputExample(text_a=data['inputs_pretokenized'], guid=idx, label=label_numeric)\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "# Few-shot sampling from the training data\n",
    "sampler = FewShotSampler(num_examples_per_label=30)\n",
    "fewshot_data = sampler(dataset['train'], seed=42)\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "roberta_path = \"/path/to/facebook_roberta-base\"\n",
    "model = RobertaForMaskedLM.from_pretrained(roberta_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(roberta_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"prefix_tuning_results_roberta.json\"\n",
    "results = []\n",
    "\n",
    "# Define hyperparameter search ranges\n",
    "learning_rates = [1e-4, 3e-5]  # Learning rates to test\n",
    "num_soft_tokens = [10, 50, 100]  # Number of soft tokens for prefix tuning\n",
    "warmup_steps = [10, 20, 25]  # Warm-up steps for learning rate scheduler\n",
    "\n",
    "# Loop through hyperparameter combinations\n",
    "for lr in learning_rates:\n",
    "    for tokens in num_soft_tokens:\n",
    "        for warmup in warmup_steps:\n",
    "            print(f\"Testing: LR={lr}, Soft Tokens={tokens}, Warm-Up Steps={warmup}\")\n",
    "\n",
    "            # Reload the model and tokenizer for each configuration\n",
    "            model = RobertaForMaskedLM.from_pretrained(roberta_path)\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(roberta_path)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            # Initialize the P-tuning template\n",
    "            template = PtuningTemplate(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                text='{\"placeholder\":\"text_a\"} {\"mask\"}',  # Template text\n",
    "                prompt_encoder_type=\"lstm\",  # Use LSTM-based prefix encoder\n",
    "            )\n",
    "\n",
    "            # Define a manual verbalizer\n",
    "            verbalizer = ManualVerbalizer(\n",
    "                tokenizer=tokenizer,\n",
    "                num_classes=2,  # Binary classification\n",
    "                label_words=[[\"positive\", \"good\", \"excellent\", \"wonderful\"], [\"negative\", \"bad\", \"horrible\", \"terrible\"]],\n",
    "                classes=[0, 1]  # Class labels\n",
    "            )\n",
    "\n",
    "            # Initialize the prompt model\n",
    "            prompt_model = PromptForClassification(\n",
    "                plm=model,\n",
    "                template=template,\n",
    "                verbalizer=verbalizer,\n",
    "                freeze_plm=True,  # Freeze the pre-trained RoBERTa model\n",
    "            )\n",
    "\n",
    "            # Create dataloaders for training and validation\n",
    "            train_dataloader = PromptDataLoader(\n",
    "                dataset=fewshot_data,\n",
    "                template=template,\n",
    "                tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=MLMTokenizerWrapper,\n",
    "                max_seq_length=480, decoder_max_length=3,\n",
    "                batch_size=5, shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "                truncate_method=\"tail\",\n",
    "            )\n",
    "            \n",
    "            validation_dataloader = PromptDataLoader(\n",
    "                dataset=dataset[\"validation\"],\n",
    "                template=template,\n",
    "                tokenizer=tokenizer,\n",
    "                tokenizer_wrapper_class=MLMTokenizerWrapper,\n",
    "                decoder_max_length=3,\n",
    "                batch_size=5, shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "                truncate_method=\"tail\",\n",
    "            )\n",
    "\n",
    "            # Prepare optimizer and unfreeze necessary parameters\n",
    "            optimizer_grouped_parameters = []\n",
    "            for name, param in prompt_model.named_parameters():\n",
    "                if not param.requires_grad and param.dtype in [torch.float32, torch.float64]:\n",
    "                    param.requires_grad = True  # Unfreeze necessary parameters\n",
    "                optimizer_grouped_parameters.append({'params': param})\n",
    "\n",
    "            # Initialize optimizer and loss function\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "            loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # Initialize learning rate scheduler\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=1000)\n",
    "\n",
    "            # Training loop\n",
    "            num_epochs = 10\n",
    "            gradient_accumulation_steps = 1  # Gradients will accumulate before the optimizer step\n",
    "\n",
    "            prompt_model.train()  # Set model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                total_loss = 0\n",
    "                pbar = tqdm(train_dataloader, desc=\"Training\")  # Progress bar\n",
    "\n",
    "                for step, inputs in enumerate(pbar):\n",
    "                    logits = prompt_model(inputs)  # Get model predictions\n",
    "                    labels = inputs['label']  # Ground-truth labels\n",
    "\n",
    "                    loss = loss_func(logits, labels)  # Compute loss\n",
    "                    loss.backward()  # Backpropagation\n",
    "                    \n",
    "                    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                        optimizer.step()  # Optimizer step\n",
    "                        scheduler.step()  # Update learning rate\n",
    "                        optimizer.zero_grad()  # Reset gradients\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    pbar.set_postfix({\"loss\": total_loss / (step + 1)})\n",
    "\n",
    "            # Define evaluation function\n",
    "            def evaluate(prompt_model, dataloader):\n",
    "                prompt_model.eval()  # Set model to evaluation mode\n",
    "                total, correct = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for inputs in dataloader:\n",
    "                        logits = prompt_model(inputs)\n",
    "                        preds = torch.argmax(logits, dim=-1)  # Predicted class\n",
    "                        labels = inputs['label']\n",
    "                        total += len(labels)\n",
    "                        correct += (preds == labels).sum().item()\n",
    "                return correct / total  # Compute accuracy\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            val_accuracy = evaluate(prompt_model, validation_dataloader)\n",
    "            print(f\"Validation Accuracy after Epoch {epoch + 1}: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Log results\n",
    "            result = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_soft_tokens\": tokens,\n",
    "                \"warmup_steps\": warmup,\n",
    "                \"final_loss\": total_loss / (10 * len(train_dataloader)),\n",
    "                \"accuracy\": val_accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Save intermediate results\n",
    "            with open(log_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "print(\"Tuning complete. Results saved to\", log_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a9581bf",
   "metadata": {},
   "source": [
    "# Overview of Prefix Tuning for Sentiment Classification\n",
    "\n",
    "This code implements a **Ptuning approach** for sentiment classification using the OpenPrompt framework and a pre-trained RoBERTa model. The script explores the application of soft prompts (Ptuning) and evaluates various hyperparameter configurations to optimize performance in a **few-shot learning setting**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Dataset Preparation**:\n",
    "   - Loads a dataset from disk and processes it into the OpenPrompt-compatible `InputExample` format.\n",
    "   - Maps sentiment labels (`positive`, `negative`) to numeric values for classification.\n",
    "   - Implements a **few-shot sampler** to create a small training set with 30 examples per label.\n",
    "\n",
    "2. **Prefix-Tuning**:\n",
    "   - Uses the `PtuningTemplate` from OpenPrompt to generate **soft prompts** encoded with an LSTM-based architecture.\n",
    "   - Dynamically adds soft tokens to the input for prefix tuning without modifying the pre-trained model weights.\n",
    "\n",
    "3. **Manual Verbalizer**:\n",
    "   - Maps predictions from the model's masked language modeling (MLM) head to the appropriate sentiment labels (`positive` or `negative`).\n",
    "   - Allows flexible and interpretable classification.\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - Tests various combinations of:\n",
    "     - **Learning rates**: `1e-4`, `3e-5`\n",
    "     - **Number of soft tokens**: `10`, `50`, `100`\n",
    "     - **Warm-up steps**: `10`, `20`, `25`\n",
    "   - Automates experimentation to identify the best configuration for prefix-tuning.\n",
    "\n",
    "5. **Training Loop**:\n",
    "   - Optimizes the prefix-tuning parameters using the AdamW optimizer.\n",
    "   - Tracks training loss and uses a learning rate scheduler for fine-grained control of learning dynamics.\n",
    "   - Supports gradient accumulation for efficient training with small batch sizes.\n",
    "\n",
    "6. **Evaluation**:\n",
    "   - Computes accuracy on the validation set after each training epoch.\n",
    "   - Provides a reusable evaluation function to compute performance metrics.\n",
    "\n",
    "7. **Result Logging**:\n",
    "   - Logs hyperparameter settings, training loss, and validation accuracy.\n",
    "   - Saves results in a JSON file for later analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "### 1. **Dataset Handling**\n",
    "- Loads the dataset and prepares it for few-shot learning by sampling a small subset of examples.\n",
    "\n",
    "### 2. **Model and Template Setup**\n",
    "- Loads a pre-trained RoBERTa model and tokenizer.\n",
    "- Configures a soft prompt with `PtuningTemplate` and defines a `ManualVerbalizer` to map model predictions to labels.\n",
    "\n",
    "### 3. **Hyperparameter Search**\n",
    "- Iterates through combinations of learning rates, soft token counts, and warm-up steps.\n",
    "- Reloads the model and tokenizer for each configuration to ensure independence between experiments.\n",
    "\n",
    "### 4. **Training**\n",
    "- Trains the prefix-tuning parameters while keeping the base RoBERTa model frozen.\n",
    "- Tracks and logs training loss for each batch.\n",
    "\n",
    "### 5. **Validation**\n",
    "- Evaluates the model's performance on the validation set after each epoch.\n",
    "- Computes validation accuracy to assess the effectiveness of each hyperparameter configuration.\n",
    "\n",
    "### 6. **Logging Results**\n",
    "- Records the results for each hyperparameter configuration in a structured JSON file.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "This code is designed for **sentiment classification** tasks in low-resource settings. The prefix-tuning approach leverages the power of pre-trained language models while keeping the computational cost low by only optimizing soft prompt parameters. It can be adapted to other classification tasks with minimal changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of Prefix Tuning\n",
    "\n",
    "1. **Efficient Fine-Tuning**:\n",
    "   - Optimizes only a small set of parameters (soft prompts) instead of the entire model.\n",
    "   - Reduces memory usage and speeds up training.\n",
    "\n",
    "2. **Few-Shot Learning**:\n",
    "   - Achieves strong performance with minimal labeled data.\n",
    "   - Suitable for scenarios where large labeled datasets are unavailable.\n",
    "\n",
    "3. **Modular and Flexible**:\n",
    "   - Easy to experiment with different templates, verbalizers, and hyperparameters.\n",
    "   - Compatible with various pre-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This implementation showcases how prefix tuning can be applied to sentiment classification tasks using OpenPrompt and RoBERTa. By automating hyperparameter tuning and supporting few-shot learning, the script provides a robust framework for experimenting with prompt-based learning approaches in NLP tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
