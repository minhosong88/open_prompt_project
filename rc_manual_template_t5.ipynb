{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b20c87-60d6-4215-8ff3-040f1b66ab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': [27, 54, 169, 48, 2458, 10, 3424, 757, 1855, 6986, 116, 3, 9, 8096, 9016, 190, 8, 2358, 13304, 406, 174, 53, 136, 827, 12, 1903, 190, 5, 100, 2906, 116, 3, 9, 8096, 6914, 45, 46, 616, 213, 34, 19, 72, 18054, 12, 46, 616, 213, 34, 19, 705, 18054, 5, 27664, 257, 19, 8, 381, 13, 14219, 13, 3, 9, 8096, 16, 3, 9, 787, 2908, 5, 1563, 31, 7, 497, 25, 27157, 3, 9, 21776, 13, 3136, 16, 3, 9, 4119, 13, 387, 5, 37, 29, 25, 27157, 192, 21776, 7, 13, 3136, 16, 430, 4119, 13, 387, 5, 37, 511, 1127, 56, 43, 3, 9, 1146, 6145, 13, 3136, 5, 852, 6, 27, 43, 3, 9, 126, 1419, 10, 71, 388, 474, 192, 12294, 6, 4119, 71, 11, 4119, 272, 6, 3353, 28, 4081, 6201, 13, 387, 30, 12, 3, 9, 953, 11, 3, 6412, 550, 12, 281, 691, 112, 4842, 5, 978, 520, 764, 590, 11, 1509, 8, 192, 12294, 11, 1500, 12, 474, 128, 2656, 16, 135, 12, 143, 3, 9, 11915, 3281, 5, 37, 861, 3, 22929, 192, 14987, 1329, 7, 13, 2656, 139, 4119, 71, 11, 386, 14987, 1329, 7, 13, 2656, 139, 4119, 272, 5, 11801, 48, 822, 754, 10, 4073, 4119, 65, 3, 9, 1146, 6145, 13, 2656, 58], 'inputs_pretokenized': \"\\nI can use this background: Passive transport occurs when a substance passes through the cell membrane without needing any energy to pass through. This happens when a substance moves from an area where it is more concentrated to an area where it is less concentrated. Concentration is the number of particles of a substance in a given volume. Let's say you dissolve a teaspoon of salt in a cup of water. Then you dissolve two teaspoons of salt in another cup of water. The second solution will have a higher concentration of salt.\\n\\nNow, I have a new situation: A man put two cups, cup A and cup B, filled with equal amounts of water on to a table and walked away to go check his mail. His son came along and saw the two cups and decided to put some sugar in them to make a tasty drink. The child poured two spoonfuls of sugar into cup A and three spoonfuls of sugar into cup B.\\n\\nAnswer this question please: Which cup has a higher concentration of sugar?\", 'targets': [4119, 272, 1], 'targets_pretokenized': '\\ncup B\\n'}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "tokenizing: 18it [00:00, 418.43it/s]\n",
      "/lustre/work/client/users/minhos/.conda/envs/prompt_learning_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/18 [00:31<?, ?it/s, loss=0.94]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average loss: 0.9402022957801819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/18 [01:21<?, ?it/s, loss=0.601]\n",
      "Training:   0%|          | 0/18 [01:21<?, ?it/s, loss=0.601]\n",
      "\n",
      "Training:   0%|          | 0/18 [00:02<?, ?it/s, loss=1.61]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.847]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, average loss: 0.8473597131669521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/18 [00:06<?, ?it/s, loss=0.637]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:09<?, ?it/s, loss=0.497]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:12<?, ?it/s, loss=0.442]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:14<?, ?it/s, loss=0.373]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:16<?, ?it/s, loss=0.437]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:18<?, ?it/s, loss=0.473]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:21<?, ?it/s, loss=0.439]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:23<?, ?it/s, loss=0.398]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:25<?, ?it/s, loss=0.386]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:27<?, ?it/s, loss=0.441]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:29<?, ?it/s, loss=0.414]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:31<?, ?it/s, loss=0.386]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:33<?, ?it/s, loss=0.371]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:36<?, ?it/s, loss=0.351]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:38<?, ?it/s, loss=0.352]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:40<?, ?it/s, loss=0.343]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:03<?, ?it/s, loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, average loss: 0.12093606404960155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/18 [00:35<?, ?it/s, loss=0.305] \n",
      "Training:   0%|          | 0/18 [00:35<?, ?it/s, loss=0.305]\n",
      "\n",
      "Training:   0%|          | 0/18 [00:02<?, ?it/s, loss=0.727]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.4]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, average loss: 0.3999467305839062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/18 [00:05<?, ?it/s, loss=0.302]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:09<?, ?it/s, loss=0.233]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:11<?, ?it/s, loss=0.193]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:13<?, ?it/s, loss=0.17] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:16<?, ?it/s, loss=0.199]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:18<?, ?it/s, loss=0.247]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:20<?, ?it/s, loss=0.234]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:22<?, ?it/s, loss=0.212]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:25<?, ?it/s, loss=0.227]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:27<?, ?it/s, loss=0.44] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:30<?, ?it/s, loss=0.407]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:32<?, ?it/s, loss=0.378]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:34<?, ?it/s, loss=0.355]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:37<?, ?it/s, loss=0.334]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:39<?, ?it/s, loss=0.321]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:41<?, ?it/s, loss=0.307]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, average loss: 0.12957225181162357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/18 [00:44<?, ?it/s, loss=0.172] \n",
      "Training:   0%|          | 0/18 [00:44<?, ?it/s, loss=0.172]\n",
      "\n",
      "Training:   0%|          | 0/18 [00:02<?, ?it/s, loss=0.234]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.124]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, average loss: 0.12358094193041325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/18 [00:07<?, ?it/s, loss=0.0878]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:09<?, ?it/s, loss=0.069] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:11<?, ?it/s, loss=0.0566]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:13<?, ?it/s, loss=0.0493]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:16<?, ?it/s, loss=0.087] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:18<?, ?it/s, loss=0.184]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:20<?, ?it/s, loss=0.164]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:23<?, ?it/s, loss=0.148]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:25<?, ?it/s, loss=0.197]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:27<?, ?it/s, loss=0.207]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:29<?, ?it/s, loss=0.192]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:31<?, ?it/s, loss=0.178]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:33<?, ?it/s, loss=0.168]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:36<?, ?it/s, loss=0.159]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:38<?, ?it/s, loss=0.16] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:40<?, ?it/s, loss=0.159]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.0742]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, average loss: 0.07416049297899008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/18 [00:43<?, ?it/s, loss=0.401] \n",
      "Training:   0%|          | 0/18 [00:43<?, ?it/s, loss=0.401]\n",
      "\n",
      "Training:   0%|          | 0/18 [00:02<?, ?it/s, loss=0.144]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.0777]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, average loss: 0.07767152041196823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/18 [00:07<?, ?it/s, loss=0.0602]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:10<?, ?it/s, loss=0.0467]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:12<?, ?it/s, loss=0.0756]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:15<?, ?it/s, loss=0.0638]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:17<?, ?it/s, loss=0.0756]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:19<?, ?it/s, loss=0.232] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:21<?, ?it/s, loss=0.214]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:24<?, ?it/s, loss=0.193]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:26<?, ?it/s, loss=0.247]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:29<?, ?it/s, loss=0.248]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:32<?, ?it/s, loss=0.234]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:34<?, ?it/s, loss=0.217]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:37<?, ?it/s, loss=0.23] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:39<?, ?it/s, loss=0.216]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:42<?, ?it/s, loss=0.207]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:44<?, ?it/s, loss=0.2]  \u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.0902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, average loss: 0.09022496757097542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/18 [00:41<?, ?it/s, loss=0.129] \n",
      "Training:   0%|          | 0/18 [00:41<?, ?it/s, loss=0.129]\n",
      "\n",
      "Training:   0%|          | 0/18 [00:02<?, ?it/s, loss=0.12]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:04<?, ?it/s, loss=0.0659]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, average loss: 0.0658742911182344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/18 [00:06<?, ?it/s, loss=0.051] \u001b[A\n",
      "Training:   0%|          | 0/18 [00:09<?, ?it/s, loss=0.0407]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:12<?, ?it/s, loss=0.0346]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:14<?, ?it/s, loss=0.0302]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:16<?, ?it/s, loss=0.0339]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:18<?, ?it/s, loss=0.0636]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:22<?, ?it/s, loss=0.0599]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:24<?, ?it/s, loss=0.0541]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:26<?, ?it/s, loss=0.0689]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:29<?, ?it/s, loss=0.0883]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:31<?, ?it/s, loss=0.0819]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:33<?, ?it/s, loss=0.0763]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:35<?, ?it/s, loss=0.0722]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:38<?, ?it/s, loss=0.0685]\u001b[A\n",
      "Training:   0%|          | 0/18 [00:40<?, ?it/s, loss=0.0648]\u001b[A\n",
      "tokenizing: 1it [00:00, 103.59it/s]:43<?, ?it/s, loss=0.0669]\u001b[A\n",
      "tokenizing: 1it [00:00, 275.36it/s]\n",
      "tokenizing: 1it [00:00, 167.85it/s]\n",
      "tokenizing: 1it [00:00, 233.55it/s]\n",
      "tokenizing: 1it [00:00, 132.14it/s]\n",
      "tokenizing: 1it [00:00, 173.92it/s]\n",
      "tokenizing: 1it [00:00, 172.89it/s]\n",
      "tokenizing: 1it [00:00, 166.88it/s]\n",
      "tokenizing: 1it [00:00, 163.62it/s]\n",
      "tokenizing: 1it [00:00, 190.32it/s]\n",
      "tokenizing: 1it [00:00, 138.56it/s]\n",
      "tokenizing: 1it [00:00, 176.68it/s]\n",
      "tokenizing: 1it [00:00, 260.10it/s]\n",
      "tokenizing: 1it [00:00, 180.82it/s]\n",
      "tokenizing: 1it [00:00, 257.97it/s]\n",
      "tokenizing: 1it [00:00, 258.03it/s]\n",
      "tokenizing: 1it [00:00, 253.86it/s]\n",
      "tokenizing: 1it [00:00, 167.16it/s]\n",
      "tokenizing: 1it [00:00, 501.29it/s]\n",
      "tokenizing: 1it [00:00, 250.05it/s]\n",
      "tokenizing: 1it [00:00, 239.31it/s]\n",
      "tokenizing: 1it [00:00, 254.66it/s]\n",
      "tokenizing: 1it [00:00, 245.86it/s]\n",
      "tokenizing: 1it [00:00, 482.83it/s]\n",
      "tokenizing: 1it [00:00, 119.87it/s]\n",
      "tokenizing: 1it [00:00, 125.08it/s]\n",
      "tokenizing: 1it [00:00, 173.96it/s]\n",
      "tokenizing: 1it [00:00, 248.12it/s]\n",
      "tokenizing: 1it [00:00, 173.56it/s]\n",
      "tokenizing: 1it [00:00, 133.23it/s]\n",
      "tokenizing: 1it [00:00, 508.77it/s]\n",
      "tokenizing: 1it [00:00, 286.11it/s]\n",
      "tokenizing: 1it [00:00, 210.83it/s]\n",
      "tokenizing: 1it [00:00, 186.19it/s]\n",
      "tokenizing: 1it [00:00, 175.80it/s]\n",
      "tokenizing: 1it [00:00, 196.49it/s]\n",
      "tokenizing: 1it [00:00, 161.47it/s]\n",
      "tokenizing: 1it [00:00, 184.99it/s]\n",
      "tokenizing: 1it [00:00, 528.18it/s]\n",
      "tokenizing: 1it [00:00, 506.37it/s]\n",
      "tokenizing: 1it [00:00, 154.21it/s]\n",
      "tokenizing: 1it [00:00, 184.07it/s]\n",
      "tokenizing: 1it [00:00, 171.31it/s]\n",
      "tokenizing: 1it [00:00, 458.69it/s]\n",
      "tokenizing: 1it [00:00, 264.21it/s]\n",
      "tokenizing: 1it [00:00, 527.78it/s]\n",
      "tokenizing: 1it [00:00, 459.75it/s]\n",
      "tokenizing: 1it [00:00, 394.83it/s]\n",
      "tokenizing: 1it [00:00, 247.06it/s]\n",
      "tokenizing: 1it [00:00, 210.39it/s]\n",
      "tokenizing: 1it [00:00, 246.91it/s]\n",
      "tokenizing: 1it [00:00, 224.47it/s]\n",
      "tokenizing: 1it [00:00, 246.74it/s]\n",
      "tokenizing: 1it [00:00, 256.88it/s]\n",
      "tokenizing: 1it [00:00, 238.75it/s]\n",
      "tokenizing: 1it [00:00, 254.60it/s]\n",
      "tokenizing: 1it [00:00, 263.15it/s]\n",
      "tokenizing: 1it [00:00, 281.10it/s]\n",
      "tokenizing: 1it [00:00, 288.53it/s]\n",
      "tokenizing: 1it [00:00, 185.77it/s]\n",
      "tokenizing: 1it [00:00, 525.21it/s]\n",
      "tokenizing: 1it [00:00, 516.79it/s]\n",
      "tokenizing: 1it [00:00, 280.35it/s]\n",
      "tokenizing: 1it [00:00, 501.59it/s]\n",
      "tokenizing: 1it [00:00, 542.39it/s]\n",
      "tokenizing: 1it [00:00, 259.21it/s]\n",
      "tokenizing: 1it [00:00, 285.37it/s]\n",
      "tokenizing: 1it [00:00, 291.07it/s]\n",
      "tokenizing: 1it [00:00, 289.16it/s]\n",
      "tokenizing: 1it [00:00, 293.53it/s]\n",
      "tokenizing: 1it [00:00, 268.95it/s]\n",
      "tokenizing: 1it [00:00, 557.60it/s]\n",
      "tokenizing: 1it [00:00, 285.85it/s]\n",
      "tokenizing: 1it [00:00, 479.29it/s]\n",
      "tokenizing: 1it [00:00, 271.88it/s]\n",
      "tokenizing: 1it [00:00, 283.71it/s]\n",
      "tokenizing: 1it [00:00, 287.68it/s]\n",
      "tokenizing: 1it [00:00, 570.96it/s]\n",
      "tokenizing: 1it [00:00, 566.11it/s]\n",
      "tokenizing: 1it [00:00, 529.85it/s]\n",
      "tokenizing: 1it [00:00, 519.35it/s]\n",
      "tokenizing: 1it [00:00, 519.93it/s]\n",
      "tokenizing: 1it [00:00, 517.30it/s]\n",
      "tokenizing: 1it [00:00, 520.58it/s]\n",
      "tokenizing: 1it [00:00, 505.76it/s]\n",
      "tokenizing: 1it [00:00, 518.84it/s]\n",
      "tokenizing: 1it [00:00, 519.74it/s]\n",
      "tokenizing: 1it [00:00, 541.27it/s]\n",
      "tokenizing: 1it [00:00, 535.67it/s]\n",
      "tokenizing: 1it [00:00, 545.85it/s]\n",
      "tokenizing: 1it [00:00, 240.75it/s]\n",
      "tokenizing: 1it [00:00, 227.28it/s]\n",
      "tokenizing: 1it [00:00, 526.00it/s]\n",
      "tokenizing: 1it [00:00, 477.44it/s]\n",
      "tokenizing: 1it [00:00, 523.96it/s]\n",
      "tokenizing: 1it [00:00, 276.67it/s]\n",
      "tokenizing: 1it [00:00, 230.49it/s]\n",
      "tokenizing: 1it [00:00, 504.73it/s]\n",
      "tokenizing: 1it [00:00, 360.68it/s]\n",
      "tokenizing: 1it [00:00, 182.29it/s]\n",
      "tokenizing: 1it [00:00, 256.83it/s]\n",
      "tokenizing: 1it [00:00, 273.78it/s]\n",
      "tokenizing: 1it [00:00, 289.24it/s]\n",
      "tokenizing: 1it [00:00, 290.02it/s]\n",
      "tokenizing: 1it [00:00, 299.02it/s]\n",
      "tokenizing: 1it [00:00, 295.25it/s]\n",
      "tokenizing: 1it [00:00, 288.17it/s]\n",
      "tokenizing: 1it [00:00, 290.79it/s]\n",
      "tokenizing: 1it [00:00, 231.27it/s]\n",
      "tokenizing: 1it [00:00, 176.02it/s]\n",
      "tokenizing: 1it [00:00, 169.40it/s]\n",
      "tokenizing: 1it [00:00, 180.81it/s]\n",
      "tokenizing: 1it [00:00, 172.90it/s]\n",
      "tokenizing: 1it [00:00, 179.31it/s]\n",
      "tokenizing: 1it [00:00, 202.45it/s]\n",
      "tokenizing: 1it [00:00, 229.42it/s]\n",
      "tokenizing: 1it [00:00, 227.72it/s]\n",
      "tokenizing: 1it [00:00, 232.91it/s]\n",
      "tokenizing: 1it [00:00, 215.18it/s]\n",
      "tokenizing: 1it [00:00, 155.13it/s]\n",
      "tokenizing: 1it [00:00, 158.05it/s]\n",
      "tokenizing: 1it [00:00, 157.39it/s]\n",
      "tokenizing: 1it [00:00, 152.36it/s]\n",
      "tokenizing: 1it [00:00, 234.16it/s]\n",
      "tokenizing: 1it [00:00, 169.87it/s]\n",
      "tokenizing: 1it [00:00, 146.86it/s]\n",
      "tokenizing: 1it [00:00, 174.89it/s]\n",
      "tokenizing: 1it [00:00, 177.35it/s]\n",
      "tokenizing: 1it [00:00, 184.88it/s]\n",
      "tokenizing: 1it [00:00, 184.46it/s]\n",
      "tokenizing: 1it [00:00, 170.81it/s]\n",
      "tokenizing: 1it [00:00, 512.94it/s]\n",
      "tokenizing: 1it [00:00, 516.29it/s]\n",
      "tokenizing: 1it [00:00, 401.22it/s]\n",
      "tokenizing: 1it [00:00, 395.50it/s]\n",
      "tokenizing: 1it [00:00, 423.80it/s]\n",
      "tokenizing: 1it [00:00, 167.54it/s]\n",
      "tokenizing: 1it [00:00, 164.41it/s]\n",
      "tokenizing: 1it [00:00, 516.86it/s]\n",
      "tokenizing: 1it [00:00, 169.20it/s]\n",
      "tokenizing: 1it [00:00, 171.41it/s]\n",
      "tokenizing: 1it [00:00, 168.85it/s]\n",
      "tokenizing: 1it [00:00, 174.70it/s]\n",
      "tokenizing: 1it [00:00, 161.84it/s]\n",
      "tokenizing: 1it [00:00, 174.43it/s]\n",
      "tokenizing: 1it [00:00, 175.63it/s]\n",
      "tokenizing: 1it [00:00, 188.06it/s]\n",
      "tokenizing: 1it [00:00, 184.78it/s]\n",
      "tokenizing: 1it [00:00, 536.29it/s]\n",
      "tokenizing: 1it [00:00, 260.10it/s]\n",
      "tokenizing: 1it [00:00, 275.76it/s]\n",
      "tokenizing: 1it [00:00, 522.78it/s]\n",
      "tokenizing: 1it [00:00, 274.75it/s]\n",
      "tokenizing: 1it [00:00, 550.07it/s]\n",
      "tokenizing: 1it [00:00, 88.62it/s]\n",
      "tokenizing: 1it [00:00, 202.11it/s]\n",
      "tokenizing: 1it [00:00, 166.47it/s]\n",
      "tokenizing: 1it [00:00, 198.85it/s]\n",
      "tokenizing: 1it [00:00, 188.41it/s]\n",
      "tokenizing: 1it [00:00, 250.06it/s]\n",
      "tokenizing: 1it [00:00, 188.26it/s]\n",
      "tokenizing: 1it [00:00, 275.65it/s]\n",
      "tokenizing: 1it [00:00, 265.33it/s]\n",
      "tokenizing: 1it [00:00, 261.07it/s]\n",
      "tokenizing: 1it [00:00, 188.99it/s]\n",
      "tokenizing: 1it [00:00, 192.82it/s]\n",
      "tokenizing: 1it [00:00, 204.67it/s]\n",
      "tokenizing: 1it [00:00, 196.79it/s]\n",
      "tokenizing: 1it [00:00, 392.69it/s]\n",
      "tokenizing: 1it [00:00, 259.21it/s]\n",
      "tokenizing: 1it [00:00, 251.80it/s]\n",
      "tokenizing: 1it [00:00, 248.96it/s]\n",
      "tokenizing: 1it [00:00, 269.73it/s]\n",
      "tokenizing: 1it [00:00, 530.59it/s]\n",
      "tokenizing: 1it [00:00, 134.08it/s]\n",
      "tokenizing: 1it [00:00, 261.93it/s]\n",
      "tokenizing: 1it [00:00, 266.42it/s]\n",
      "tokenizing: 1it [00:00, 137.43it/s]\n",
      "tokenizing: 1it [00:00, 219.14it/s]\n",
      "tokenizing: 1it [00:00, 208.47it/s]\n",
      "tokenizing: 1it [00:00, 233.51it/s]\n",
      "tokenizing: 1it [00:00, 230.68it/s]\n",
      "tokenizing: 1it [00:00, 231.03it/s]\n",
      "tokenizing: 1it [00:00, 233.22it/s]\n",
      "tokenizing: 1it [00:00, 627.42it/s]\n",
      "tokenizing: 1it [00:00, 151.99it/s]\n",
      "tokenizing: 1it [00:00, 665.66it/s]\n",
      "tokenizing: 1it [00:00, 624.15it/s]\n",
      "tokenizing: 1it [00:00, 577.17it/s]\n",
      "tokenizing: 1it [00:00, 578.21it/s]\n",
      "tokenizing: 1it [00:00, 551.45it/s]\n",
      "tokenizing: 1it [00:00, 541.48it/s]\n",
      "tokenizing: 1it [00:00, 524.29it/s]\n",
      "tokenizing: 1it [00:00, 535.19it/s]\n",
      "tokenizing: 1it [00:00, 534.92it/s]\n",
      "tokenizing: 1it [00:00, 494.79it/s]\n",
      "tokenizing: 1it [00:00, 541.90it/s]\n",
      "tokenizing: 1it [00:00, 482.44it/s]\n",
      "tokenizing: 1it [00:00, 265.03it/s]\n",
      "tokenizing: 1it [00:00, 284.42it/s]\n",
      "tokenizing: 1it [00:00, 289.24it/s]\n",
      "tokenizing: 1it [00:00, 196.65it/s]\n",
      "tokenizing: 1it [00:00, 260.79it/s]\n",
      "tokenizing: 1it [00:00, 271.60it/s]\n",
      "tokenizing: 1it [00:00, 285.68it/s]\n",
      "tokenizing: 1it [00:00, 585.80it/s]\n",
      "tokenizing: 1it [00:00, 305.53it/s]\n",
      "tokenizing: 1it [00:00, 263.40it/s]\n",
      "tokenizing: 1it [00:00, 298.38it/s]\n",
      "tokenizing: 1it [00:00, 241.82it/s]\n",
      "tokenizing: 1it [00:00, 300.19it/s]\n",
      "tokenizing: 1it [00:00, 262.16it/s]\n",
      "tokenizing: 1it [00:00, 197.62it/s]\n",
      "tokenizing: 1it [00:00, 576.62it/s]\n",
      "tokenizing: 1it [00:00, 286.55it/s]\n",
      "tokenizing: 1it [00:00, 312.84it/s]\n",
      "tokenizing: 1it [00:00, 320.74it/s]\n",
      "tokenizing: 1it [00:00, 315.76it/s]\n",
      "tokenizing: 1it [00:00, 226.66it/s]\n",
      "tokenizing: 1it [00:00, 244.45it/s]\n",
      "tokenizing: 1it [00:00, 239.29it/s]\n",
      "tokenizing: 1it [00:00, 248.70it/s]\n",
      "tokenizing: 1it [00:00, 246.29it/s]\n",
      "tokenizing: 1it [00:00, 243.91it/s]\n",
      "tokenizing: 1it [00:00, 242.56it/s]\n",
      "tokenizing: 1it [00:00, 244.95it/s]\n",
      "tokenizing: 1it [00:00, 161.23it/s]\n",
      "tokenizing: 1it [00:00, 168.52it/s]\n",
      "tokenizing: 1it [00:00, 163.94it/s]\n",
      "tokenizing: 1it [00:00, 164.89it/s]\n",
      "tokenizing: 1it [00:00, 167.47it/s]\n",
      "tokenizing: 1it [00:00, 165.44it/s]\n",
      "tokenizing: 1it [00:00, 240.54it/s]\n",
      "tokenizing: 1it [00:00, 482.77it/s]\n",
      "tokenizing: 1it [00:00, 166.34it/s]\n",
      "tokenizing: 1it [00:00, 171.98it/s]\n",
      "tokenizing: 1it [00:00, 161.46it/s]\n",
      "tokenizing: 1it [00:00, 155.25it/s]\n",
      "tokenizing: 1it [00:00, 160.73it/s]\n",
      "tokenizing: 1it [00:00, 228.37it/s]\n",
      "tokenizing: 1it [00:00, 162.20it/s]\n",
      "tokenizing: 1it [00:00, 158.97it/s]\n",
      "tokenizing: 1it [00:00, 459.50it/s]\n",
      "tokenizing: 1it [00:00, 214.07it/s]\n",
      "tokenizing: 1it [00:00, 158.57it/s]\n",
      "tokenizing: 1it [00:00, 344.36it/s]\n",
      "tokenizing: 1it [00:00, 166.93it/s]\n",
      "tokenizing: 1it [00:00, 242.08it/s]\n",
      "tokenizing: 1it [00:00, 266.46it/s]\n",
      "tokenizing: 1it [00:00, 264.34it/s]\n",
      "tokenizing: 1it [00:00, 270.34it/s]\n",
      "tokenizing: 1it [00:00, 264.91it/s]\n",
      "tokenizing: 1it [00:00, 279.62it/s]\n",
      "tokenizing: 1it [00:00, 189.25it/s]\n",
      "tokenizing: 1it [00:00, 194.15it/s]\n",
      "tokenizing: 1it [00:00, 290.46it/s]\n",
      "tokenizing: 1it [00:00, 277.69it/s]\n",
      "tokenizing: 1it [00:00, 283.53it/s]\n",
      "tokenizing: 1it [00:00, 282.62it/s]\n",
      "tokenizing: 1it [00:00, 270.23it/s]\n",
      "tokenizing: 1it [00:00, 269.73it/s]\n",
      "tokenizing: 1it [00:00, 189.33it/s]\n",
      "tokenizing: 1it [00:00, 544.86it/s]\n",
      "tokenizing: 1it [00:00, 181.18it/s]\n",
      "tokenizing: 1it [00:00, 193.18it/s]\n",
      "tokenizing: 1it [00:00, 195.67it/s]\n",
      "tokenizing: 1it [00:00, 191.87it/s]\n",
      "tokenizing: 1it [00:00, 500.81it/s]\n",
      "tokenizing: 1it [00:00, 539.95it/s]\n",
      "tokenizing: 1it [00:00, 519.93it/s]\n",
      "tokenizing: 1it [00:00, 189.82it/s]\n",
      "tokenizing: 1it [00:00, 145.78it/s]\n",
      "tokenizing: 1it [00:00, 177.99it/s]\n",
      "tokenizing: 1it [00:00, 167.94it/s]\n",
      "tokenizing: 1it [00:00, 181.45it/s]\n",
      "tokenizing: 1it [00:00, 179.30it/s]\n",
      "tokenizing: 1it [00:00, 105.34it/s]\n",
      "tokenizing: 1it [00:00, 179.37it/s]\n",
      "tokenizing: 1it [00:00, 270.32it/s]\n",
      "tokenizing: 1it [00:00, 266.25it/s]\n",
      "tokenizing: 1it [00:00, 259.73it/s]\n",
      "tokenizing: 1it [00:00, 458.74it/s]\n",
      "tokenizing: 1it [00:00, 141.64it/s]\n",
      "tokenizing: 1it [00:00, 181.26it/s]\n",
      "tokenizing: 1it [00:00, 180.94it/s]\n",
      "tokenizing: 1it [00:00, 179.88it/s]\n",
      "tokenizing: 1it [00:00, 510.75it/s]\n",
      "tokenizing: 1it [00:00, 170.15it/s]\n",
      "tokenizing: 1it [00:00, 181.18it/s]\n",
      "tokenizing: 1it [00:00, 234.19it/s]\n",
      "tokenizing: 1it [00:00, 185.26it/s]\n",
      "tokenizing: 1it [00:00, 185.30it/s]\n",
      "tokenizing: 1it [00:00, 200.71it/s]\n",
      "tokenizing: 1it [00:00, 140.37it/s]\n",
      "tokenizing: 1it [00:00, 137.77it/s]\n",
      "tokenizing: 1it [00:00, 187.19it/s]\n",
      "tokenizing: 1it [00:00, 187.42it/s]\n",
      "tokenizing: 1it [00:00, 172.40it/s]\n",
      "tokenizing: 1it [00:00, 180.80it/s]\n",
      "tokenizing: 1it [00:00, 178.74it/s]\n",
      "tokenizing: 1it [00:00, 189.00it/s]\n",
      "tokenizing: 1it [00:00, 181.34it/s]\n",
      "tokenizing: 1it [00:00, 253.20it/s]\n",
      "tokenizing: 1it [00:00, 246.06it/s]\n",
      "tokenizing: 1it [00:00, 169.75it/s]\n",
      "tokenizing: 1it [00:00, 182.42it/s]\n",
      "tokenizing: 1it [00:00, 487.31it/s]\n",
      "tokenizing: 1it [00:00, 134.24it/s]\n",
      "tokenizing: 1it [00:00, 134.93it/s]\n",
      "tokenizing: 1it [00:00, 526.13it/s]\n",
      "tokenizing: 1it [00:00, 268.14it/s]\n",
      "tokenizing: 1it [00:00, 128.79it/s]\n",
      "tokenizing: 1it [00:00, 137.22it/s]\n",
      "tokenizing: 1it [00:00, 547.85it/s]\n",
      "tokenizing: 1it [00:00, 138.99it/s]\n",
      "tokenizing: 1it [00:00, 191.29it/s]\n",
      "tokenizing: 1it [00:00, 218.52it/s]\n",
      "tokenizing: 1it [00:00, 215.46it/s]\n",
      "tokenizing: 1it [00:00, 277.46it/s]\n",
      "tokenizing: 1it [00:00, 213.81it/s]\n",
      "tokenizing: 1it [00:00, 531.87it/s]\n",
      "tokenizing: 1it [00:00, 275.71it/s]\n",
      "tokenizing: 1it [00:00, 289.84it/s]\n",
      "tokenizing: 1it [00:00, 302.95it/s]\n",
      "tokenizing: 1it [00:00, 261.64it/s]\n",
      "tokenizing: 1it [00:00, 518.01it/s]\n",
      "tokenizing: 1it [00:00, 173.00it/s]\n",
      "tokenizing: 1it [00:00, 183.42it/s]\n",
      "tokenizing: 1it [00:00, 180.08it/s]\n",
      "tokenizing: 1it [00:00, 172.34it/s]\n",
      "tokenizing: 1it [00:00, 174.35it/s]\n",
      "tokenizing: 1it [00:00, 499.80it/s]\n",
      "tokenizing: 1it [00:00, 512.88it/s]\n",
      "tokenizing: 1it [00:00, 261.88it/s]\n",
      "tokenizing: 1it [00:00, 263.31it/s]\n",
      "tokenizing: 1it [00:00, 176.59it/s]\n",
      "tokenizing: 1it [00:00, 225.20it/s]\n",
      "tokenizing: 1it [00:00, 198.67it/s]\n",
      "tokenizing: 1it [00:00, 447.92it/s]\n",
      "tokenizing: 1it [00:00, 235.05it/s]\n",
      "tokenizing: 1it [00:00, 247.41it/s]\n",
      "tokenizing: 1it [00:00, 429.74it/s]\n",
      "tokenizing: 1it [00:00, 233.34it/s]\n",
      "tokenizing: 1it [00:00, 243.61it/s]\n",
      "tokenizing: 1it [00:00, 250.30it/s]\n",
      "tokenizing: 1it [00:00, 386.54it/s]\n",
      "tokenizing: 1it [00:00, 492.17it/s]\n",
      "tokenizing: 1it [00:00, 247.82it/s]\n",
      "tokenizing: 1it [00:00, 253.91it/s]\n",
      "tokenizing: 1it [00:00, 138.38it/s]\n",
      "tokenizing: 1it [00:00, 504.67it/s]\n",
      "tokenizing: 1it [00:00, 335.87it/s]\n",
      "tokenizing: 1it [00:00, 133.59it/s]\n",
      "tokenizing: 1it [00:00, 193.79it/s]\n",
      "tokenizing: 1it [00:00, 173.19it/s]\n",
      "tokenizing: 1it [00:00, 191.43it/s]\n",
      "tokenizing: 1it [00:00, 487.77it/s]\n",
      "tokenizing: 1it [00:00, 198.42it/s]\n",
      "tokenizing: 1it [00:00, 197.01it/s]\n",
      "tokenizing: 1it [00:00, 204.49it/s]\n",
      "tokenizing: 1it [00:00, 438.73it/s]\n",
      "tokenizing: 1it [00:00, 273.60it/s]\n",
      "tokenizing: 1it [00:00, 400.72it/s]\n",
      "tokenizing: 1it [00:00, 138.07it/s]\n",
      "tokenizing: 1it [00:00, 192.59it/s]\n",
      "tokenizing: 1it [00:00, 187.94it/s]\n",
      "tokenizing: 1it [00:00, 189.25it/s]\n",
      "tokenizing: 1it [00:00, 182.04it/s]\n",
      "tokenizing: 1it [00:00, 186.17it/s]\n",
      "tokenizing: 1it [00:00, 188.56it/s]\n",
      "tokenizing: 1it [00:00, 186.49it/s]\n",
      "tokenizing: 1it [00:00, 188.90it/s]\n",
      "tokenizing: 1it [00:00, 197.89it/s]\n",
      "tokenizing: 1it [00:00, 167.81it/s]\n",
      "tokenizing: 1it [00:00, 160.66it/s]\n",
      "tokenizing: 1it [00:00, 183.99it/s]\n",
      "tokenizing: 1it [00:00, 169.03it/s]\n",
      "tokenizing: 1it [00:00, 183.85it/s]\n",
      "tokenizing: 1it [00:00, 172.42it/s]\n",
      "tokenizing: 1it [00:00, 138.86it/s]\n",
      "tokenizing: 1it [00:00, 129.08it/s]\n",
      "tokenizing: 1it [00:00, 179.11it/s]\n",
      "tokenizing: 1it [00:00, 172.84it/s]\n",
      "tokenizing: 1it [00:00, 137.98it/s]\n",
      "tokenizing: 1it [00:00, 185.97it/s]\n",
      "tokenizing: 1it [00:00, 203.76it/s]\n",
      "tokenizing: 1it [00:00, 193.79it/s]\n",
      "tokenizing: 1it [00:00, 200.60it/s]\n",
      "tokenizing: 1it [00:00, 116.99it/s]\n",
      "tokenizing: 1it [00:00, 113.78it/s]\n",
      "tokenizing: 1it [00:00, 125.47it/s]\n",
      "tokenizing: 1it [00:00, 118.55it/s]\n",
      "tokenizing: 1it [00:00, 121.85it/s]\n",
      "tokenizing: 1it [00:00, 126.46it/s]\n",
      "tokenizing: 1it [00:00, 239.83it/s]\n",
      "tokenizing: 1it [00:00, 216.51it/s]\n",
      "tokenizing: 1it [00:00, 145.40it/s]\n",
      "tokenizing: 1it [00:00, 150.77it/s]\n",
      "tokenizing: 1it [00:00, 154.92it/s]\n",
      "tokenizing: 1it [00:00, 157.62it/s]\n",
      "tokenizing: 1it [00:00, 222.62it/s]\n",
      "tokenizing: 1it [00:00, 261.10it/s]\n",
      "tokenizing: 1it [00:00, 265.08it/s]\n",
      "tokenizing: 1it [00:00, 180.34it/s]\n",
      "tokenizing: 1it [00:00, 161.01it/s]\n",
      "tokenizing: 1it [00:00, 239.05it/s]\n",
      "tokenizing: 1it [00:00, 126.54it/s]\n",
      "tokenizing: 1it [00:00, 139.99it/s]\n",
      "tokenizing: 1it [00:00, 530.19it/s]\n",
      "tokenizing: 1it [00:00, 516.60it/s]\n",
      "tokenizing: 1it [00:00, 271.30it/s]\n",
      "tokenizing: 1it [00:00, 184.67it/s]\n",
      "tokenizing: 1it [00:00, 182.55it/s]\n",
      "tokenizing: 1it [00:00, 194.62it/s]\n",
      "tokenizing: 1it [00:00, 181.47it/s]\n",
      "tokenizing: 1it [00:00, 203.46it/s]\n",
      "tokenizing: 1it [00:00, 172.44it/s]\n",
      "tokenizing: 1it [00:00, 178.73it/s]\n",
      "tokenizing: 1it [00:00, 182.16it/s]\n",
      "tokenizing: 1it [00:00, 161.75it/s]\n",
      "tokenizing: 1it [00:00, 166.69it/s]\n",
      "tokenizing: 1it [00:00, 171.15it/s]\n",
      "tokenizing: 1it [00:00, 177.31it/s]\n",
      "tokenizing: 1it [00:00, 197.30it/s]\n",
      "tokenizing: 1it [00:00, 208.59it/s]\n",
      "tokenizing: 1it [00:00, 195.21it/s]\n",
      "tokenizing: 1it [00:00, 204.46it/s]\n",
      "tokenizing: 1it [00:00, 203.12it/s]\n",
      "tokenizing: 1it [00:00, 204.87it/s]\n",
      "tokenizing: 1it [00:00, 197.92it/s]\n",
      "tokenizing: 1it [00:00, 241.62it/s]\n",
      "tokenizing: 1it [00:00, 190.81it/s]\n",
      "tokenizing: 1it [00:00, 193.15it/s]\n",
      "tokenizing: 1it [00:00, 146.45it/s]\n",
      "tokenizing: 1it [00:00, 137.85it/s]\n",
      "tokenizing: 1it [00:00, 155.79it/s]\n",
      "tokenizing: 1it [00:00, 159.53it/s]\n",
      "tokenizing: 1it [00:00, 153.93it/s]\n",
      "tokenizing: 1it [00:00, 155.86it/s]\n",
      "tokenizing: 1it [00:00, 136.10it/s]\n",
      "tokenizing: 1it [00:00, 153.61it/s]\n",
      "tokenizing: 1it [00:00, 157.64it/s]\n",
      "tokenizing: 1it [00:00, 406.42it/s]\n",
      "tokenizing: 1it [00:00, 130.61it/s]\n",
      "tokenizing: 1it [00:00, 174.73it/s]\n",
      "tokenizing: 1it [00:00, 130.19it/s]\n",
      "tokenizing: 1it [00:00, 162.11it/s]\n",
      "tokenizing: 1it [00:00, 119.37it/s]\n",
      "tokenizing: 1it [00:00, 180.42it/s]\n",
      "tokenizing: 1it [00:00, 177.85it/s]\n",
      "tokenizing: 1it [00:00, 264.93it/s]\n",
      "tokenizing: 1it [00:00, 522.20it/s]\n",
      "tokenizing: 1it [00:00, 187.81it/s]\n",
      "tokenizing: 1it [00:00, 236.49it/s]\n",
      "tokenizing: 1it [00:00, 144.67it/s]\n",
      "tokenizing: 1it [00:00, 84.16it/s]\n",
      "tokenizing: 1it [00:00, 159.04it/s]\n",
      "tokenizing: 1it [00:00, 220.06it/s]\n",
      "tokenizing: 1it [00:00, 213.69it/s]\n",
      "tokenizing: 1it [00:00, 205.96it/s]\n",
      "tokenizing: 1it [00:00, 224.37it/s]\n",
      "tokenizing: 1it [00:00, 214.09it/s]\n",
      "tokenizing: 1it [00:00, 219.37it/s]\n",
      "tokenizing: 1it [00:00, 431.69it/s]\n",
      "tokenizing: 1it [00:00, 222.36it/s]\n",
      "tokenizing: 1it [00:00, 213.91it/s]\n",
      "tokenizing: 1it [00:00, 154.71it/s]\n",
      "tokenizing: 1it [00:00, 452.85it/s]\n",
      "tokenizing: 1it [00:00, 232.94it/s]\n",
      "tokenizing: 1it [00:00, 197.98it/s]\n",
      "tokenizing: 1it [00:00, 222.90it/s]\n",
      "tokenizing: 1it [00:00, 234.08it/s]\n",
      "tokenizing: 1it [00:00, 440.35it/s]\n",
      "tokenizing: 1it [00:00, 501.59it/s]\n",
      "tokenizing: 1it [00:00, 263.58it/s]\n",
      "tokenizing: 1it [00:00, 258.52it/s]\n",
      "tokenizing: 1it [00:00, 258.84it/s]\n",
      "tokenizing: 1it [00:00, 261.87it/s]\n",
      "tokenizing: 1it [00:00, 250.44it/s]\n",
      "tokenizing: 1it [00:00, 269.31it/s]\n",
      "tokenizing: 1it [00:00, 243.06it/s]\n",
      "tokenizing: 1it [00:00, 256.08it/s]\n",
      "tokenizing: 1it [00:00, 264.91it/s]\n",
      "tokenizing: 1it [00:00, 254.88it/s]\n",
      "tokenizing: 1it [00:00, 259.45it/s]\n",
      "tokenizing: 1it [00:00, 270.69it/s]\n",
      "tokenizing: 1it [00:00, 474.90it/s]\n",
      "tokenizing: 1it [00:00, 488.85it/s]\n",
      "tokenizing: 1it [00:00, 468.79it/s]\n",
      "tokenizing: 1it [00:00, 323.58it/s]\n",
      "tokenizing: 1it [00:00, 186.90it/s]\n",
      "tokenizing: 1it [00:00, 174.98it/s]\n",
      "tokenizing: 1it [00:00, 176.45it/s]\n",
      "tokenizing: 1it [00:00, 175.17it/s]\n",
      "tokenizing: 1it [00:00, 178.88it/s]\n",
      "tokenizing: 1it [00:00, 267.32it/s]\n",
      "tokenizing: 1it [00:00, 267.73it/s]\n",
      "tokenizing: 1it [00:00, 256.14it/s]\n",
      "tokenizing: 1it [00:00, 177.85it/s]\n",
      "tokenizing: 1it [00:00, 178.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8820\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from openprompt.prompts import ManualTemplate, MixedTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from random import shuffle\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "dataset_path = \"/lustre/work/client/users/minhos/cache/datasets/p3_ropes_rc\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    raw_dataset[split] = raw_dataset[split].select(range(500))\n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        dataset[split].append(data)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "\n",
    "# Load the T5 model\n",
    "from openprompt.plms import load_plm\n",
    "t5_path = \"/lustre/work/client/users/minhos/models_for_supercomputer/t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"qa_manual_template_multi_binary_t5.json\"\n",
    "results = []\n",
    "\n",
    "# For Training, select 18 samples with the same context and different questions\n",
    "data1 = dataset['train'][2:20]\n",
    "dataset1 = []\n",
    "for idx, data in enumerate(data1):\n",
    "    question = data[\"inputs_pretokenized\"]\n",
    "    correct_answer = data[\"targets_pretokenized\"].strip()\n",
    "    label = 0 if correct_answer in ['cell X', 'Cell A', 'larger', 'more'] else 1  \n",
    "    # Create an InputExample\n",
    "    input_example = InputExample(\n",
    "        text_a=question,\n",
    "        label=label,\n",
    "        guid=idx, # Assign a dummy label since there is only one answer\n",
    "        meta={\"correct_answer\": correct_answer}\n",
    "    )\n",
    "    dataset1.append(input_example)\n",
    "\n",
    "template1 = ManualTemplate(\n",
    "    tokenizer=tokenizer,\n",
    "    text='{\"placeholder\":\"text_a\"} The answer is: {\"mask\"}',\n",
    ")\n",
    "verbalizer1 = ManualVerbalizer(\n",
    "    tokenizer=tokenizer,\n",
    "    num_classes=2,\n",
    "    label_words=[['cell X','cell A', 'larger','more'],['cell Z', 'cell B', 'smaller', 'less']]\n",
    ")\n",
    "\n",
    "prompt_model = PromptForClassification(\n",
    "    plm=model,\n",
    "    template=template1,\n",
    "    verbalizer=verbalizer1,\n",
    "    freeze_plm=False,\n",
    ")\n",
    "train_dataloader = PromptDataLoader(\n",
    "    dataset = dataset1,\n",
    "    template=template1,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "    decoder_max_length=68, max_seq_length=480,\n",
    "    batch_size=1)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "from tqdm import tqdm\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=0.0001)\n",
    "prompt_model.train()\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        pbar.set_postfix({\"loss\": tot_loss / (step + 1)})\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch+1, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "\n",
    "data2 = dataset['validation'][:1000]\n",
    "dataset2 = []\n",
    "for idx, data in enumerate(data2):\n",
    "    question = data[\"inputs_pretokenized\"]\n",
    "    correct_answer = data[\"targets_pretokenized\"].strip()\n",
    "    # Create an InputExample\n",
    "    input_example = InputExample(\n",
    "        text_a=question,\n",
    "        label= 0,\n",
    "        guid=idx, # Assign a dummy label since there is only one answer\n",
    "        meta={\"correct_answer\": correct_answer}\n",
    "    )\n",
    "    dataset2.append(input_example)\n",
    "    \n",
    "for idx, data in enumerate(dataset2):\n",
    "\n",
    "    template2 = ManualTemplate(\n",
    "        tokenizer=tokenizer,\n",
    "        text='{\"placeholder\":\"text_a\"} The answer is: {\"mask\"}',\n",
    "    )\n",
    "    \n",
    "    verbalizer2 = ManualVerbalizer(\n",
    "        tokenizer=tokenizer,\n",
    "        num_classes=2,\n",
    "        label_words=[[data.meta['correct_answer']],[\"other\"]]\n",
    "    )\n",
    "\n",
    "    prompt_model.template = template2\n",
    "    prompt_model.verbalizer = verbalizer2\n",
    "\n",
    "    validation_dataloader = PromptDataLoader(\n",
    "        dataset = [data],\n",
    "        template=template2,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "        decoder_max_length=3, max_seq_length=480,\n",
    "        batch_size=1\n",
    "        )\n",
    "\n",
    "    prompt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in validation_dataloader:\n",
    "            logits = prompt_model(inputs)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct = preds.item() == data.label\n",
    "    \n",
    "    results.append({\"index\": idx, \"correct\": correct})\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save results to JSON\n",
    "with open(log_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a761c-8cb6-4856-b319-53c27755c7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d7ead-a577-4567-8694-b701d2d1ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
