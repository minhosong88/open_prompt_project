{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b20c87-60d6-4215-8ff3-040f1b66ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import InputExample\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/path/to/your/dataset\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    raw_dataset[split] = raw_dataset[split].select(range(500))  # Select the first 500 examples\n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        dataset[split].append(data)\n",
    "\n",
    "# Print the first training example for debugging\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "t5_path = \"/path/to/t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "# Logging setup\n",
    "log_file = \"rc_binary_t5.json\"\n",
    "results = []\n",
    "\n",
    "# Prepare training dataset\n",
    "# For the training, 'ropes_background_new_situation_answer' from P3 dataset.\n",
    "# These 18 samples share the same background.\n",
    "\n",
    "data1 = dataset['train'][2:20]  # Select 18 samples\n",
    "dataset1 = []\n",
    "for idx, data in enumerate(data1):\n",
    "    question = data[\"inputs_pretokenized\"]  # Extract the question\n",
    "    correct_answer = data[\"targets_pretokenized\"].strip()  # Extract the correct answer\n",
    "    label = 0 if correct_answer in ['cell X', 'Cell A', 'larger', 'more'] else 1  # Binary label\n",
    "    input_example = InputExample(\n",
    "        text_a=question,\n",
    "        label=label,\n",
    "        guid=idx,\n",
    "        meta={\"correct_answer\": correct_answer}\n",
    "    )\n",
    "    dataset1.append(input_example)\n",
    "\n",
    "# Define manual template and verbalizer for training\n",
    "template1 = ManualTemplate(\n",
    "    tokenizer=tokenizer,\n",
    "    text='{\"placeholder\":\"text_a\"} The answer is: {\"mask\"}',\n",
    ")\n",
    "verbalizer1 = ManualVerbalizer(\n",
    "    tokenizer=tokenizer,\n",
    "    num_classes=2,\n",
    "    label_words=[['cell X', 'cell A', 'larger', 'more'], ['cell Z', 'cell B', 'smaller', 'less']]\n",
    ")\n",
    "\n",
    "# Initialize the prompt model\n",
    "prompt_model = PromptForClassification(\n",
    "    plm=model,\n",
    "    template=template1,\n",
    "    verbalizer=verbalizer1,\n",
    "    freeze_plm=False,\n",
    ")\n",
    "\n",
    "# Prepare training dataloader\n",
    "train_dataloader = PromptDataLoader(\n",
    "    dataset=dataset1,\n",
    "    template=template1,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "    decoder_max_length=68, max_seq_length=480,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=0.0001)\n",
    "prompt_model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    tot_loss = 0\n",
    "    pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        logits = prompt_model(inputs)  # Get predictions\n",
    "        labels = inputs['label']  # Ground-truth labels\n",
    "        loss = loss_func(logits, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()  # Update weights\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        pbar.set_postfix({\"loss\": tot_loss / (step + 1)})\n",
    "\n",
    "# Prepare validation dataset\n",
    "data2 = dataset['validation'][:1000]  # Use first 1000 examples for validation\n",
    "dataset2 = []\n",
    "for idx, data in enumerate(data2):\n",
    "    question = data[\"inputs_pretokenized\"]\n",
    "    correct_answer = data[\"targets_pretokenized\"].strip()\n",
    "    input_example = InputExample(\n",
    "        text_a=question,\n",
    "        label=0,  # Assign a dummy label for validation\n",
    "        guid=idx,\n",
    "        meta={\"correct_answer\": correct_answer}\n",
    "    )\n",
    "    dataset2.append(input_example)\n",
    "\n",
    "# Validation loop\n",
    "for idx, data in enumerate(dataset2):\n",
    "    template2 = ManualTemplate(\n",
    "        tokenizer=tokenizer,\n",
    "        text='{\"placeholder\":\"text_a\"} The answer is: {\"mask\"}',\n",
    "    )\n",
    "    verbalizer2 = ManualVerbalizer(\n",
    "        tokenizer=tokenizer,\n",
    "        num_classes=2,\n",
    "        label_words=[[data.meta['correct_answer']], [\"other\"]]\n",
    "    )\n",
    "    prompt_model.template = template2  # Update template\n",
    "    prompt_model.verbalizer = verbalizer2  # Update verbalizer\n",
    "\n",
    "    validation_dataloader = PromptDataLoader(\n",
    "        dataset=[data],\n",
    "        template=template2,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "        decoder_max_length=3, max_seq_length=480,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    prompt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in validation_dataloader:\n",
    "            logits = prompt_model(inputs)\n",
    "            preds = torch.argmax(logits, dim=-1)  # Predicted class\n",
    "            correct = preds.item() == data.label  # Compare with true label\n",
    "\n",
    "    results.append({\"index\": idx, \"correct\": correct})\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save results to JSON file\n",
    "with open(log_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0470a0e7",
   "metadata": {},
   "source": [
    "# Overview of Reading Comprehension Task with T5 and OpenPrompt Framework\n",
    "\n",
    "This code implements a **reading comprehension** task using the OpenPrompt framework and a pre-trained **T5 model**. The task is reframed as a **binary classification problem**, where the model predicts whether the provided answer is correct (`right answer`) or incorrect (`wrong answer`). The implementation explores both **zero-shot** and **few-shot learning**, examining the model's ability to generalize and learn from a small number of training examples. It also highlights how fine-tuning can affect the performance of pre-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### 1. **Task Framing as Binary Classification**\n",
    "- The reading comprehension task, typically requiring exact answer generation or selection, is reformulated as a binary classification problem:\n",
    "  - **Right Answer**: The model predicts the correct answer from a predefined set.\n",
    "  - **Wrong Answer**: Any answer not matching the correct label is classified as incorrect.\n",
    "\n",
    "### 2. **Zero-Shot and Few-Shot Learning**\n",
    "- **Zero-Shot**: Evaluates the pre-trained T5 model without any additional training to establish baseline performance.\n",
    "- **Few-Shot**: Fine-tunes the T5 model on a small dataset of 18 samples with the same context but different questions to test its ability to learn patterns from limited data.\n",
    "\n",
    "### 3. **Manual Template and Verbalizer**\n",
    "- **Template**: A manual template structures the input text into a prompt, such as:\n",
    "  ```python\n",
    "  {\"placeholder\":\"text_a\"} The answer is: {\"mask\"}\n",
    "  ```\n",
    "- **Verbalizer**: A manual verbalizer maps model outputs to binary labels (`correct` or `incorrect`) using predefined label words.\n",
    "\n",
    "### 4. **Training and Validation**\n",
    "- **Few-Shot Training**: Fine-tunes both the T5 model and the manual template using a carefully curated set of examples.\n",
    "- **Dynamic Validation**: During validation, the verbalizer dynamically updates to include the correct answer for each specific example, ensuring accurate evaluation.\n",
    "\n",
    "### 5. **Results Logging**\n",
    "- Tracks and logs the accuracy of the model on the validation set after fine-tuning.\n",
    "- Saves predictions and correctness information for individual validation samples to a JSON file.\n",
    "\n",
    "---\n",
    "\n",
    "## Observations and Limitations\n",
    "\n",
    "1. **Few-Shot Learning Benefits**:\n",
    " - Few-shot learning improves T5's performance on the binary classification task, demonstrating its ability to learn structural patterns from limited data.\n",
    "\n",
    "2. **Impact on Pre-Trained Models**:\n",
    " - Fine-tuning can `confuse pre-trained models in some cases`, reducing their baseline performance. This highlights the need for careful monitoring during training to ensure consistent improvements.\n",
    "\n",
    "3. **Simplification of the Task**:\n",
    " - The reading comprehension task is simplified into binary classification. While this approach is efficient, it might not capture the nuanced understanding required for more complex tasks, such as ranking or reasoning between multiple answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **Reading Comprehension**: Adaptation of pre-trained models like T5 for reading comprehension tasks using prompt-based learning.\n",
    "- **Few-Shot Learning**: Demonstrates the potential of a small number of examples to influence model performance on structured tasks.\n",
    "- **Binary Classification**: Illustrates how complex NLP tasks can be reframed into simpler classification problems for ease of implementation and experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This implementation highlights the flexibility of the OpenPrompt framework and the adaptability of T5 for reading comprehension tasks reframed as binary classification. The results show the potential of few-shot learning to enhance performance, while also cautioning about the possibility of confusing pre-trained models through fine-tuning. This approach provides an efficient and flexible framework for experimenting with prompt-based learning in reading comprehension and similar NLP tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
