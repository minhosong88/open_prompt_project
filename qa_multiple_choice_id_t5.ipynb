{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd2e21-dc41-46f7-a294-0728ae25c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "from datasets import load_from_disk\n",
    "from openprompt.pipeline_base import PromptDataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from random import shuffle\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"/path/to/your/data/set\"\n",
    "raw_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "t5_path = \"/path/to/t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "# Set up logging\n",
    "log_file = \"qamc_id_t5.json\"\n",
    "results = []\n",
    "\n",
    "# Map textual labels to numeric values\n",
    "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4}\n",
    "\n",
    "# Prepare datasets for training and validation\n",
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    if split == 'train':\n",
    "        # Shuffle and select a subset for training\n",
    "        raw_dataset[split] = raw_dataset[split].shuffle(seed=42).select(range(1000))\n",
    "    else:\n",
    "        # Select a subset for validation\n",
    "        raw_dataset[split] = raw_dataset[split].select(range(500))\n",
    "    \n",
    "    for idx, data in enumerate(raw_dataset[split]):\n",
    "        label_text = data[\"targets_pretokenized\"].strip()  # Extract the correct answer\n",
    "        label_numeric = label_map.get(label_text, -1)  # Convert to numeric label\n",
    "        input_example = InputExample(text_a=data['inputs_pretokenized'], guid=idx, label=label_numeric)\n",
    "        dataset[split].append(input_example)\n",
    "\n",
    "# Print a sample InputExample and its type for verification\n",
    "print(dataset['train'][0])\n",
    "print(type(dataset['train'][0]))\n",
    "\n",
    "# Few-shot sampling from the training data\n",
    "sampler = FewShotSampler(num_examples_per_label=30)\n",
    "fewshot_data = sampler(dataset['train'], seed=42)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(prompt_model, dataloader):\n",
    "    prompt_model.eval()  # Set the model to evaluation mode\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in dataloader:\n",
    "            logits = prompt_model(inputs)\n",
    "            preds = torch.argmax(logits, dim=-1)  # Predicted class\n",
    "            labels = inputs['label']  # True labels\n",
    "            \n",
    "            total += len(labels)\n",
    "            correct += (preds == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Hyperparameter ranges for grid search\n",
    "learning_rates = [0.005, 0.001, 0.0005]  # Learning rates to test\n",
    "warmup_steps = [10]  # Warm-up steps for scheduler\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "for lr in learning_rates:\n",
    "    for warmup in warmup_steps:\n",
    "\n",
    "        # Reload model and tokenizer for each configuration\n",
    "        model = T5ForConditionalGeneration.from_pretrained(t5_path)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "\n",
    "        # Define the manual template for input formatting\n",
    "        template = ManualTemplate(\n",
    "            tokenizer=tokenizer,\n",
    "            text='{\"placeholder\":\"text_a\"} Which option is correct? {\"mask\"}',\n",
    "        )\n",
    "\n",
    "        # Define the verbalizer to map model predictions to labels\n",
    "        verbalizer = ManualVerbalizer(\n",
    "            tokenizer=tokenizer,\n",
    "            num_classes=5,  # Five options (A, B, C, D, E)\n",
    "            label_words=[\n",
    "                [\"A\", \"a\", \"Option A\", \"first choice\"],\n",
    "                [\"B\", \"b\", \"Option B\", \"second choice\"],\n",
    "                [\"C\", \"c\", \"Option C\", \"third choice\"],\n",
    "                [\"D\", \"d\", \"Option D\", \"fourth choice\"],\n",
    "                [\"E\", \"e\", \"Option E\", \"fifth choice\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Wrap one example for debugging (optional)\n",
    "        wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "\n",
    "        # Initialize the prompt model\n",
    "        prompt_model = PromptForClassification(\n",
    "            plm=model,\n",
    "            template=template,\n",
    "            verbalizer=verbalizer,\n",
    "            freeze_plm=False,  # Allow fine-tuning of T5 model\n",
    "        )\n",
    "\n",
    "        # Prepare data loaders for training and validation\n",
    "        train_dataloader = PromptDataLoader(\n",
    "            dataset=fewshot_data,\n",
    "            template=template,\n",
    "            tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "            decoder_max_length=3, max_seq_length=480,\n",
    "            batch_size=5\n",
    "        )\n",
    "\n",
    "        validation_dataloader = PromptDataLoader(\n",
    "            dataset=dataset[\"validation\"],\n",
    "            template=template,\n",
    "            tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=T5TokenizerWrapper,\n",
    "            decoder_max_length=3, max_seq_length=480,\n",
    "            batch_size=20\n",
    "        )\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Set optimizer parameters and no decay for biases and LayerNorm weights\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=1000)\n",
    "\n",
    "        # Training loop\n",
    "        prompt_model.train()\n",
    "        for epoch in range(10):  # 10 epochs\n",
    "            total_loss = 0\n",
    "            pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "            for step, inputs in enumerate(train_dataloader):\n",
    "                logits = prompt_model(inputs)  # Forward pass\n",
    "                labels = inputs['label']  # Ground-truth labels\n",
    "                loss = loss_func(logits, labels)  # Compute loss\n",
    "                loss.backward()  # Backpropagation\n",
    "                total_loss += loss.item()\n",
    "                optimizer.step()  # Update parameters\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                pbar.set_postfix({\"loss\": total_loss / (step + 1)})\n",
    "\n",
    "        # Validate the model after each epoch\n",
    "        val_accuracy = evaluate(prompt_model, validation_dataloader)\n",
    "        print(f\"Validation Accuracy after Epoch {epoch + 1}: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Log results\n",
    "        result = {\n",
    "            \"learning_rate\": lr,\n",
    "            \"warmup_steps\": warmup,\n",
    "            \"final_loss\": total_loss / (10 * len(train_dataloader)),\n",
    "            \"accuracy\": val_accuracy\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        # Save results to a JSON file\n",
    "        with open(log_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Tuning complete. Results saved to\", log_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51c98f9e",
   "metadata": {},
   "source": [
    "# Overview of QA Task Implementation with T5\n",
    "\n",
    "This code demonstrates the implementation of a **question-answering multiple-choice (QA-MC)** task using the OpenPrompt framework and a pre-trained **T5 model**. The focus is on fine-tuning the T5 model for multiple-choice classification tasks by utilizing manual templates and verbalizers. The script includes hyperparameter tuning, training, and evaluation, with detailed logging of results for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### 1. **Dataset Preparation**\n",
    "- **Loading and Preprocessing**:\n",
    "  - Loads a dataset from disk and prepares it for classification.\n",
    "  - Labels (A, B, C, D, E) are mapped to numeric values for model training.\n",
    "- **Few-Shot Sampling**:\n",
    "  - Creates a balanced training set with 30 examples per label, simulating a low-resource scenario.\n",
    "\n",
    "### 2. **Manual Template**\n",
    "- Formats the input text to include a task-specific prompt:\n",
    "  ```python\n",
    "  {\"placeholder\":\"text_a\"} Which option is correct? {\"mask\"}\n",
    "  ``` \n",
    "- Guides the model to generate predictions by appending the placeholder text with a mask token for output generation.\n",
    "\n",
    "### 3. **Manual Verbalizer**\n",
    "- Maps the model's output logits to corresponding labels (`A`, `B`, `C`, `D`, `E`).\n",
    "- Allows flexibility in label representation using synonyms like `[\"Option A\", \"first choice\"]`.\n",
    "\n",
    "### 4. **Training Process**\n",
    "- Fine-tunes both the T5 model and the manual template using:\n",
    "- **AdamW Optimizer**: Updates model parameters during backpropagation.\n",
    "- **Linear Learning Rate Scheduler**: Adjusts the learning rate dynamically with warm-up steps.\n",
    "- Logs training loss and evaluates performance at the end of each epoch.\n",
    "\n",
    "### 5. **Evaluation**\n",
    "- Computes the model's accuracy on a validation set after every epoch.\n",
    "- Compares predicted labels with ground-truth labels to measure performance.\n",
    "\n",
    "### 6. **Hyperparameter Tuning**\n",
    "- Explores different combinations of:\n",
    "- **Learning Rates**: `0.005`, `0.001`, `0.0005`.\n",
    "- **Warm-Up Steps**: `10`.\n",
    "- Identifies the best-performing configuration for fine-tuning.\n",
    "\n",
    "### 7. **Results Logging**\n",
    "- Records key metrics such as:\n",
    "- Final loss\n",
    "- Validation accuracy\n",
    "- Hyperparameter settings\n",
    "- Saves results in a JSON file for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of QA Tasks in the OpenPrompt Framework\n",
    "\n",
    "### 1. **Label Dependency**\n",
    "- QA tasks often have dynamic labels (e.g., options `A`, `B`, `C`, `D`, `E`), which require a manually defined verbalizer.\n",
    "- OpenPrompt does not inherently support dynamically generated label mappings, making it less flexible for certain QA tasks.\n",
    "\n",
    "### 2. **Output Limitations**\n",
    "- The OpenPrompt framework generates outputs based on the verbalizer's predefined label words. For QA tasks, this can lead to:\n",
    "  - Difficulty in capturing nuanced differences between options.\n",
    "  - Over-reliance on exact label word matches.\n",
    "\n",
    "### 3. **Scaling Issues**\n",
    "- Large-scale QA datasets with diverse label structures may not fit well into OpenPrompt's manual verbalizer approach, which requires significant manual effort.\n",
    "\n",
    "### 4. **Training Overhead**\n",
    "- Fine-tuning both the template and the T5 model introduces additional computational overhead, which might not be optimal for lightweight deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "### 1. **Dataset Handling**\n",
    "- The dataset is split into training and validation sets, with a maximum of 1000 examples for training and 500 for validation.\n",
    "- Each example is converted into OpenPrompt's `InputExample` format.\n",
    "\n",
    "### 2. **Template and Verbalizer Setup**\n",
    "- A manual template and verbalizer are defined to structure inputs and map predictions, respectively.\n",
    "- The template ensures compatibility with the T5 model's input-output format.\n",
    "\n",
    "### 3. **Training**\n",
    "- The T5 model is fine-tuned along with the template parameters for 10 epochs.\n",
    "- Loss is computed using cross-entropy, and parameters are optimized using AdamW.\n",
    "\n",
    "### 4. **Evaluation**\n",
    "- Accuracy is calculated on the validation set after each epoch to monitor performance.\n",
    "\n",
    "### 5. **Logging**\n",
    "- Training loss, validation accuracy, and hyperparameter configurations are logged and saved for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **QA Multiple-Choice Tasks**: Solves tasks where the model predicts the correct option from a predefined list (e.g., `A`, `B`, `C`, `D`, `E`).\n",
    "- **Few-Shot Learning**: Demonstrates how OpenPrompt can handle low-resource scenarios effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This implementation provides a practical example of fine-tuning a T5 model for QA tasks using the OpenPrompt framework. While it effectively applies templates and verbalizers to guide model predictions, limitations like dynamic label dependency and scaling challenges highlight areas where the OpenPrompt framework may struggle with QA tasks.\n",
    "\n",
    "This script is ideal for experimenting with prompt-based learning but requires manual intervention for handling dynamic and large-scale QA datasets. Further enhancements could include automating verbalizer generation or exploring more scalable prompt engineering techniques.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
